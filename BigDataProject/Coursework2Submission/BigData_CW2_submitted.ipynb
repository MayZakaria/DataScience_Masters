{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Employee Satisfaction with Apache PySpark and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Topic Proposal:\n",
    "\n",
    "### 1.1 Introduction \n",
    "\n",
    "Our project is a predictive machine learning project on a dataset containing information about employee satisfaction, considering multiple factors such as salary, years of experience, education level, and many other factors. The dataset focuses mainly on specific types of employees such as data scientists, data engineers, database administrators, and data/business analysts. However, this analysis can be applied to any type of employee, considering many factors to measure their satisfaction.\n",
    "In addition, we found this dataset interesting as data-related jobs have recently become the most important, and many companies seek data specialists and would like to measure their satisfaction considering the current international market.\n",
    "\n",
    "During our analysis, we used Apache PySpark and the Spark ML library (data frame/dataset-based) to build predictive machine learning models. Apache PySpark and Spark ML library are used in conjunction with Hadoop distributed systems. This means that if we had a larger dataset of the same data, our project would still be applicable as we could store this large dataset on Hadoop and then use the Apache Spark data processing engine and run the same Spark Machine Learning models saved with these large datasets without any modification in the code. In other words, Spark ML models can be trained, saved, and reloaded in different Spark instances for use in predictions. As is standard in the industry, the model will eventually need to be retrained with new data to maintain its predictive power depending on the application. Some models must be retrained daily, others every month, and others only every few months. In our report, we have used Random Forest, Naive Bayes, and Logistic Regression models. This report could help companies avoid the financial and operational costs associated with high employee turnover by using our predictive models so that they can take steps to enhance employee satisfaction and retention.\n",
    "\n",
    "\n",
    "On the other side, if the models are used for risk management, the predictive model's accuracy may decrease over time due to the changes in the workplace caused by proactive measures taken to increase employee satisfaction. For instance, when the company addresses the dissatisfaction factors and starts to improve these factors, it will modify the data patterns on which the model relies, leading to less accurate predictions. Therefore, to mitigate this risk, this company should retrain its models regularly based on the evolving data which leads to new actions to be considered to increase employee satisfaction. As a result, the Spark ML models remain effective in enhancing employee satisfaction and retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dataset\n",
    "\n",
    "\n",
    "The original dataset can be found at: https://www.kaggle.com/datasets/phuchuynguyen/datarelated-developers-survey-by-stack-overflow \n",
    "\n",
    "\n",
    "Our dataset consists of 13 features, a mix of numerical and categorical features, and a binary target variable indicating whether the employee is satisfied, leading to a binary classification task. This dataset is processed from the Stack Overflow Annual Survey results from 2017 to 2020, however, after the EDA and data preprocessing phases we ended up analyzing 2019 and 2020 when the COVID-19 virus appeared. We will find in the provided URL above that there are two files: survey_final.csv and processed_data_toDummies.csv. The first file \"survey_final.csv\" contains the original dataset that comes from the Stack overflow Annual Developers Survey, only considering the respondents who considered themselves already in a data-related job (Data Scientist, Machine Learning Specialist, Database Administrator, Data Analyst, Business Analyst, and Data Engineer). The second file \"processed_data_toDummies.csv,‚Äù which we have proceeded with in our project, contains the data pre-processed from the original dataset with all the developer types converted into dummy variables:\n",
    "\n",
    "\n",
    "* Data Scientist or Machine Learning Specialist\n",
    "* Database Administrator\n",
    "* Data Analyst\n",
    "* Business Analyst and Data Engineer\n",
    "\n",
    "\n",
    "Before describing our dataset features we added the \"Region\" feature to our dataset because we had 180 countries so we have decided to categorize these countries into 5 regions only: Africa, Europe, America, Asia, and Oceania. Encoding 180 countries is redundant and may lead to a reduction in the accuracy of our models. Each row in the dataset consists of 15 features.\n",
    "\n",
    "\n",
    "* *Year* (integer): the year in which data or survey responses were collected\n",
    "* *Hobbyist* (string): contains binary data (e.g., \"Yes\" or \"No\") indicating whether the individual considers programming or coding as a hobby\n",
    "* *ConvertedComp* (double): the annual compensation (salary) of the respondents, in the local currency of their respective countries\n",
    "* *Country* (string): the location information of the survey respondents, indicating where they are located or working\n",
    "* *EdLevel* (string): the highest level of education attained by the respondents, such as \"Bachelor's degree\" or \"Master's degree\"\n",
    "* *Employment* (string): describes the employment status or type of job arrangement the respondents have, which can include options like \"Full-time,\" \"Part-time,\" \"Freelance,\" or \"Self-employed.\"\n",
    "* *JobSat* (integer): describes the job satisfaction level of the respondents\n",
    "* *OrgSize* (string): the size of the organization or company where the respondents work, which could be categorized by the number of employees\n",
    "* *UndergradMajor* (string): the respondents' undergraduate majors or fields of study\n",
    "* *YearsCodePro* (integer): the number of years the respondents have been coding or working in a professional coding/programming capacity\n",
    "* *Data scientist or machine learning specialist*, *Database administrator*, *Data or business analyst*, *Engineer, data* (integer): these columns used to describe the *Developer type* column in our original dataset \"survey_final.csv\" and then converted to dummy variables. For instance, when *Database administrator* contains 1 as a value, the rest will be 0 which indicates this respondent works as a Database administrator\n",
    "* *Region* (string): the geographic region or area where the respondents are located, which is broader than a specific country\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This survey is conducted every year, leading to larger datasets that are suitable for big data analysis. However, we may need to change the EDA and data pre-processing phases as new features may be added. In other words, our project is applicable on the other collected surveys starting from 2021 till 2023 and these surveys can be found at: https://insights.stackoverflow.com/survey.\n",
    "\n",
    "\n",
    "We started by transferring the downloaded dataset to HDFS as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Copy the data from my local disk to my cluster\n",
    "# scp processed_data_toDummies.csv mzaka001@lena.doc.gold.ac.uk:/home/mzaka001/CW2/CW2_dataset\n",
    "# scp survey_final.csv  mzaka001@lena.doc.gold.ac.uk:/home/mzaka001/CW2/CW2_dataset\n",
    "\n",
    "# # Copy the dataset from local onto HDFS\n",
    "# hadoop fs -copyFromLocal CW2_dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf \n",
    "from pyspark.ml.feature import VectorAssembler, OneHotEncoder, MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from pyspark.sql import SQLContext\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.tree import RandomForest, GradientBoostedTrees\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vector\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "import random\n",
    "import time\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql.functions import when, lit\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Spark SQL Context\n",
    "sc = SparkContext(appName=\"bigdata_cw2\")\n",
    "sq = SQLContext(sc)\n",
    "\n",
    "# set spark.driver.maxResultsize to 0 to prevent memory issues\n",
    "conf = SparkConf().set(\"spark.driver.maxResultSize\",  \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to stop the spark context at the end of the session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = sq.read.csv(\"hdfs:///user/mzaka001/CW2_dataset/processed_data_toDummies.csv\", sep=',', header='true', \n",
    "                       inferSchema='true')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown below, the columns and their types that have been described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Hobbyist: string (nullable = true)\n",
      " |-- ConvertedComp: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- EdLevel: string (nullable = true)\n",
      " |-- Employment: string (nullable = true)\n",
      " |-- JobSat: integer (nullable = true)\n",
      " |-- OrgSize: string (nullable = true)\n",
      " |-- UndergradMajor: string (nullable = true)\n",
      " |-- YearsCodePro: integer (nullable = true)\n",
      " |-- Data scientist or machine learning specialist: integer (nullable = true)\n",
      " |-- Database administrator: integer (nullable = true)\n",
      " |-- Data or business analyst: integer (nullable = true)\n",
      " |-- Engineer, data: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice from the above schema that the categorical variables are: \"Hobbyist,‚Äù \"Country,‚Äù \"EdLevel,‚Äù \"Employment,‚Äù \"OrgSize,‚Äù \"UndergradMajor,‚Äù and \"Region.‚Äù \n",
    "\n",
    "\n",
    "Below are some basic descriptive statistics for our dataset: count, mean, standard deviation, minimum, and maximum. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------\n",
      " summary                                       | count                \n",
      " Year                                          | 33601                \n",
      " Hobbyist                                      | 33601                \n",
      " ConvertedComp                                 | 33601                \n",
      " Country                                       | 33601                \n",
      " EdLevel                                       | 33138                \n",
      " Employment                                    | 33561                \n",
      " JobSat                                        | 33526                \n",
      " OrgSize                                       | 31904                \n",
      " UndergradMajor                                | 30515                \n",
      " YearsCodePro                                  | 33518                \n",
      " Data scientist or machine learning specialist | 33601                \n",
      " Database administrator                        | 33601                \n",
      " Data or business analyst                      | 31037                \n",
      " Engineer, data                                | 20246                \n",
      " Region                                        | 33601                \n",
      "-RECORD 1-------------------------------------------------------------\n",
      " summary                                       | mean                 \n",
      " Year                                          | 2018.7596797714355   \n",
      " Hobbyist                                      | null                 \n",
      " ConvertedComp                                 | 62593.09253052934    \n",
      " Country                                       | null                 \n",
      " EdLevel                                       | null                 \n",
      " Employment                                    | null                 \n",
      " JobSat                                        | 6.124261766986816    \n",
      " OrgSize                                       | null                 \n",
      " UndergradMajor                                | null                 \n",
      " YearsCodePro                                  | 8.755713348051794    \n",
      " Data scientist or machine learning specialist | 0.3216570935388828   \n",
      " Database administrator                        | 0.5401029731257998   \n",
      " Data or business analyst                      | 0.3215194767535522   \n",
      " Engineer, data                                | 0.3035661365207942   \n",
      " Region                                        | null                 \n",
      "-RECORD 2-------------------------------------------------------------\n",
      " summary                                       | stddev               \n",
      " Year                                          | 0.8955979339866176   \n",
      " Hobbyist                                      | null                 \n",
      " ConvertedComp                                 | 49024.452803646236   \n",
      " Country                                       | null                 \n",
      " EdLevel                                       | null                 \n",
      " Employment                                    | null                 \n",
      " JobSat                                        | 1.9262798546449842   \n",
      " OrgSize                                       | null                 \n",
      " UndergradMajor                                | null                 \n",
      " YearsCodePro                                  | 7.1951220926673525   \n",
      " Data scientist or machine learning specialist | 0.46711915137181714  \n",
      " Database administrator                        | 0.4983965731805599   \n",
      " Data or business analyst                      | 0.46706715960891204  \n",
      " Engineer, data                                | 0.45980885163473006  \n",
      " Region                                        | null                 \n",
      "-RECORD 3-------------------------------------------------------------\n",
      " summary                                       | min                  \n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | No                   \n",
      " ConvertedComp                                 | 0.176185582          \n",
      " Country                                       | Afghanistan          \n",
      " EdLevel                                       | Associate degree     \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 0                    \n",
      " OrgSize                                       | 1,000 to 4,999 em... \n",
      " UndergradMajor                                | Another engineeri... \n",
      " YearsCodePro                                  | 1                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | 0                    \n",
      " Engineer, data                                | 0                    \n",
      " Region                                        | Africa               \n",
      "-RECORD 4-------------------------------------------------------------\n",
      " summary                                       | max                  \n",
      " Year                                          | 2020                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 299436.0             \n",
      " Country                                       | Zimbabwe             \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Independent contr... \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | Just me - I am a ... \n",
      " UndergradMajor                                | Web development o... \n",
      " YearsCodePro                                  | 30                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | 1                    \n",
      " Engineer, data                                | 1                    \n",
      " Region                                        | not found            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.describe().select(\"summary\",\n",
    " 'Year',\n",
    " 'Hobbyist',\n",
    " 'ConvertedComp',\n",
    " 'Country',\n",
    " 'EdLevel',\n",
    " 'Employment',\n",
    " 'JobSat',\n",
    " 'OrgSize',\n",
    " 'UndergradMajor',\n",
    " 'YearsCodePro',\n",
    " 'Data scientist or machine learning specialist',\n",
    " 'Database administrator',\n",
    " 'Data or business analyst',\n",
    " 'Engineer, data',\n",
    " 'Region').show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a sample of records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 43750.0              \n",
      " Country                                       | United Kingdom       \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 4                    \n",
      " OrgSize                                       | 2 to 9 employees     \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 2                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 1-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 51282.05128          \n",
      " Country                                       | Denmark              \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed part-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 100 to 499 employees \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 3                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 2-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | No                   \n",
      " ConvertedComp                                 | 25000.0              \n",
      " Country                                       | Israel               \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 6                    \n",
      " OrgSize                                       | 5,000 to 9,999 em... \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 4                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Asia                 \n",
      "-RECORD 3-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 100000.0             \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 5                    \n",
      " OrgSize                                       | 20 to 99 employees   \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 15                   \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 4-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 27000.0              \n",
      " Country                                       | Ukraine              \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 7                    \n",
      " OrgSize                                       | 100 to 499 employees \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 5                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 5-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 120000.0             \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 20 to 99 employees   \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 5                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 6-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | No                   \n",
      " ConvertedComp                                 | 62121.21212          \n",
      " Country                                       | Canada               \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 4                    \n",
      " OrgSize                                       | 20 to 99 employees   \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 10                   \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 7-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | No                   \n",
      " ConvertedComp                                 | 54545.45455          \n",
      " Country                                       | Canada               \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 8                    \n",
      " OrgSize                                       | 1,000 to 4,999 em... \n",
      " UndergradMajor                                | Computer programm... \n",
      " YearsCodePro                                  | 9                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 8-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 1481.481481          \n",
      " Country                                       | Poland               \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 2 to 9 employees     \n",
      " UndergradMajor                                | Humanities           \n",
      " YearsCodePro                                  | 1                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 9-------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 50000.0              \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 2 to 9 employees     \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 16                   \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 10------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 50000.0              \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 1                    \n",
      " OrgSize                                       | 10 to 19 employees   \n",
      " UndergradMajor                                | Another engineeri... \n",
      " YearsCodePro                                  | 3                    \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 11------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 103000.0             \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 8                    \n",
      " OrgSize                                       | 500 to 999 employees \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 16                   \n",
      " Data scientist or machine learning specialist | 0                    \n",
      " Database administrator                        | 1                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 12------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | No                   \n",
      " ConvertedComp                                 | 81000.0              \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 1,000 to 4,999 em... \n",
      " UndergradMajor                                | Another engineeri... \n",
      " YearsCodePro                                  | 7                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 13------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 120000.0             \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 10                   \n",
      " OrgSize                                       | 20 to 99 employees   \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 20                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 14------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 40000.0              \n",
      " Country                                       | Germany              \n",
      " EdLevel                                       | Secondary school     \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 6                    \n",
      " OrgSize                                       | 20 to 99 employees   \n",
      " UndergradMajor                                | null                 \n",
      " YearsCodePro                                  | 4                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 15------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 150000.0             \n",
      " Country                                       | United Kingdom       \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 8                    \n",
      " OrgSize                                       | 1,000 to 4,999 em... \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 18                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 16------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 46969.69697          \n",
      " Country                                       | Canada               \n",
      " EdLevel                                       | Some college/univ... \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 7                    \n",
      " OrgSize                                       | 1,000 to 4,999 em... \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 17                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "-RECORD 17------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 62365.5914           \n",
      " Country                                       | Austria              \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 3                    \n",
      " OrgSize                                       | 100 to 499 employees \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 11                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 18------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, I program as... \n",
      " ConvertedComp                                 | 62500.0              \n",
      " Country                                       | United Kingdom       \n",
      " EdLevel                                       | Master's degree      \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 7                    \n",
      " OrgSize                                       | 2 to 9 employees     \n",
      " UndergradMajor                                | Natural science      \n",
      " YearsCodePro                                  | 15                   \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | Europe               \n",
      "-RECORD 19------------------------------------------------------------\n",
      " Year                                          | 2017                 \n",
      " Hobbyist                                      | Yes, both            \n",
      " ConvertedComp                                 | 52500.0              \n",
      " Country                                       | United States        \n",
      " EdLevel                                       | Bachelor's degree    \n",
      " Employment                                    | Employed full-time   \n",
      " JobSat                                        | 5                    \n",
      " OrgSize                                       | 500 to 999 employees \n",
      " UndergradMajor                                | Computer science     \n",
      " YearsCodePro                                  | 4                    \n",
      " Data scientist or machine learning specialist | 1                    \n",
      " Database administrator                        | 0                    \n",
      " Data or business analyst                      | null                 \n",
      " Engineer, data                                | null                 \n",
      " Region                                        | America              \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 EDA\n",
    "\n",
    "\n",
    "In this section, we explore our dataset. First, we check if the dataset contains null values; then, for the numerical features, we replace the null values with the mean corresponding to each column, and for the categorical features we will replace the null values with the mode corresponding to each column. As we can see below some columns do not have null values such as \"Year,‚Äù \"Hobbyist,‚Äù \"ConvertedComp,‚Äù \"Country,‚Äù and \"Region\" so we will focus on the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Year': 0,\n",
       " 'Hobbyist': 0,\n",
       " 'ConvertedComp': 0,\n",
       " 'Country': 0,\n",
       " 'EdLevel': 463,\n",
       " 'Employment': 40,\n",
       " 'JobSat': 75,\n",
       " 'OrgSize': 1697,\n",
       " 'UndergradMajor': 3086,\n",
       " 'YearsCodePro': 83,\n",
       " 'Data scientist or machine learning specialist': 0,\n",
       " 'Database administrator': 0,\n",
       " 'Data or business analyst': 2564,\n",
       " 'Engineer, data': 13355,\n",
       " 'Region': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_counts = {c:dataframe.filter(dataframe[c].isNull()).count() for c in dataframe.columns}\n",
    "null_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in our dataset is: 33601\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of records in our dataset is:\", dataframe.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will start with the categorical features, we got the mode of \"EdLevel\", \"Employment\", \"OrgSize\", and \"UndergradMajor\" and then replaced the null values of each column with the corresponding mode values we have extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode_value_EdLevel:  Bachelor's degree\n",
      "mode_value_Employment:  Employed full-time\n",
      "mode_value_OrgSize:  20 to 99 employees\n",
      "mode_value_UndergradMajor:  Computer science\n"
     ]
    }
   ],
   "source": [
    "mode_EdLevel = dataframe.groupBy('EdLevel').count().orderBy(col('count').desc()).first()\n",
    "mode_Employment = dataframe.groupBy('Employment').count().orderBy(col('count').desc()).first()\n",
    "mode_OrgSize = dataframe.groupBy('OrgSize').count().orderBy(col('count').desc()).first()\n",
    "mode_UndergradMajor = dataframe.groupBy('UndergradMajor').count().orderBy(col('count').desc()).first()\n",
    "\n",
    "\n",
    "# The mode of each variable now contains the mode value and its count, however, we are interested only in the actual value\n",
    "mode_value_EdLevel = mode_EdLevel['EdLevel']\n",
    "mode_value_Employment = mode_Employment['Employment']\n",
    "mode_value_OrgSize = mode_OrgSize['OrgSize']\n",
    "mode_value_UndergradMajor = mode_UndergradMajor['UndergradMajor']\n",
    "print(\"mode_value_EdLevel: \",mode_value_EdLevel)\n",
    "print(\"mode_value_Employment: \",mode_value_Employment)\n",
    "print(\"mode_value_OrgSize: \",mode_value_OrgSize)\n",
    "print(\"mode_value_UndergradMajor: \",mode_value_UndergradMajor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the null values of each categorical column with the corresponding mode values we have extracted above\n",
    "dataframe = dataframe.na.fill(value=\"Bachelor's degree\",subset=[\"EdLevel\"])\n",
    "dataframe = dataframe.na.fill(value=\"Employed full-time\",subset=[\"Employment\"])\n",
    "dataframe = dataframe.na.fill(value=\"20 to 99 employees\",subset=[\"OrgSize\"])\n",
    "dataframe = dataframe.na.fill(value=\"Computer science\",subset=[\"UndergradMajor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the null values of \"YearsCodePro\" feature with the mean value we got from the summary statistics above\n",
    "dataframe = dataframe.na.fill(value=8.755713348051794,subset=[\"YearsCodePro\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't change the \"JobSat\" feature as this is our target label. Regarding the dummy variables, we will drop the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Year': 0,\n",
       " 'Hobbyist': 0,\n",
       " 'ConvertedComp': 0,\n",
       " 'Country': 0,\n",
       " 'EdLevel': 0,\n",
       " 'Employment': 0,\n",
       " 'JobSat': 75,\n",
       " 'OrgSize': 0,\n",
       " 'UndergradMajor': 0,\n",
       " 'YearsCodePro': 0,\n",
       " 'Data scientist or machine learning specialist': 0,\n",
       " 'Database administrator': 0,\n",
       " 'Data or business analyst': 2564,\n",
       " 'Engineer, data': 13355,\n",
       " 'Region': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the null values per column\n",
    "null_counts = {c:dataframe.filter(dataframe[c].isNull()).count() for c in dataframe.columns}\n",
    "null_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records in our dataset after drop:\n",
      "20230\n"
     ]
    }
   ],
   "source": [
    "dataframe_cleaned=dataframe.dropna()\n",
    "print(\"Total number of records in our dataset after drop:\")\n",
    "print(dataframe_cleaned.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, the total number of records we had before dropping the rows with null values was 33601 and after dropping the rows that contained null values and replacing the null values of numerical and categorical features with mean and mode respectively, the total number of records became 20230.\n",
    "\n",
    "We have stated above that we have added the \"Region\" column to ease our task, instead of proceeding with 180 countries to gain more significant results so no need to keep the \"Country\" column as we will not use it during our process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Hobbyist',\n",
       " 'ConvertedComp',\n",
       " 'EdLevel',\n",
       " 'Employment',\n",
       " 'JobSat',\n",
       " 'OrgSize',\n",
       " 'UndergradMajor',\n",
       " 'YearsCodePro',\n",
       " 'Data scientist or machine learning specialist',\n",
       " 'Database administrator',\n",
       " 'Data or business analyst',\n",
       " 'Engineer, data',\n",
       " 'Region']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop 'Country' column from the predictors since we have categorized the countries by their region\n",
    "dataframe_cleaned = dataframe_cleaned.drop(dataframe_cleaned.Country)\n",
    "dataframe_cleaned.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have created a new column called \"Target\" whose value is calculated based on the values in the \"JobSat\" column. If the \"JobSat\" column value is greater than 5,  the \"Target\" column is assigned to 1, which means \"Satisfied.\" Otherwise, the \"Target\" column is assigned to 0, which means \"Not Satisfied.\" Then we dropped the \"JobSat\" feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Hobbyist',\n",
       " 'ConvertedComp',\n",
       " 'EdLevel',\n",
       " 'Employment',\n",
       " 'OrgSize',\n",
       " 'UndergradMajor',\n",
       " 'YearsCodePro',\n",
       " 'Data scientist or machine learning specialist',\n",
       " 'Database administrator',\n",
       " 'Data or business analyst',\n",
       " 'Engineer, data',\n",
       " 'Region',\n",
       " 'Target']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned = dataframe_cleaned.withColumn(\n",
    "    \"Target\",\n",
    "    when(dataframe_cleaned[\"JobSat\"] > 5, 1)  # Satisfied\n",
    "    .otherwise(0)  # Not Satisfied\n",
    ")\n",
    "# drop 'JosSat' column from the predictors\n",
    "dataframe_cleaned = dataframe_cleaned.drop(dataframe_cleaned.JobSat)\n",
    "dataframe_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             EdLevel|\n",
      "+--------------------+\n",
      "|   Bachelor's degree|\n",
      "|Primary/elementar...|\n",
      "|Some college/univ...|\n",
      "|I never completed...|\n",
      "| Professional degree|\n",
      "|     Master's degree|\n",
      "|    Associate degree|\n",
      "|    Secondary school|\n",
      "|     Doctoral degree|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_cleaned.select(\"EdLevel\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"EdLevel\" column has \"I prefer not to answer\" as a value which is an insignificant answer so we will drop the rows containing this value. It has other values such as: \"I never completed any formal education\" and \"Some college/university study without earning a bachelor's degree\" which we can consider these answers as \"Undergraduate\". Therefore we created a new column called \"EducationLevel\" whose value is calculated based on the \"EdLevel\" column. If the \"EdLevel\" column value is \"Some college/university study without earning a bachelor's degree\" or \"I never completed any formal education\", the \"EducationLevel\" column is assigned to \"Undergraduate\".\" Otherwise, the \"EducationLevel\" column keeps the same value of \"EdLevel\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             EdLevel|\n",
      "+--------------------+\n",
      "|   Bachelor's degree|\n",
      "|Primary/elementar...|\n",
      "|Some college/univ...|\n",
      "|I never completed...|\n",
      "| Professional degree|\n",
      "|     Master's degree|\n",
      "|    Associate degree|\n",
      "|    Secondary school|\n",
      "|     Doctoral degree|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe_cleaned = dataframe_cleaned.filter(dataframe_cleaned.EdLevel != \"I prefer not to answer\")\n",
    "dataframe_cleaned.select(\"EdLevel\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Hobbyist',\n",
       " 'ConvertedComp',\n",
       " 'Employment',\n",
       " 'OrgSize',\n",
       " 'UndergradMajor',\n",
       " 'YearsCodePro',\n",
       " 'Data scientist or machine learning specialist',\n",
       " 'Database administrator',\n",
       " 'Data or business analyst',\n",
       " 'Engineer, data',\n",
       " 'Region',\n",
       " 'Target',\n",
       " 'EducationLevel']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned = dataframe_cleaned.withColumn(\n",
    "    \"EducationLevel\",\n",
    "    when(dataframe_cleaned[\"EdLevel\"].isin(\"Some college/university study without earning a bachelor's degree\", \"I never completed any formal education\"), \"Undergraduate\")\n",
    "    .otherwise(dataframe_cleaned[\"EdLevel\"]) \n",
    ")\n",
    "# drop 'EdLevel' column from the predictors\n",
    "dataframe_cleaned = dataframe_cleaned.drop(dataframe_cleaned.EdLevel)\n",
    "dataframe_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      EducationLevel|\n",
      "+--------------------+\n",
      "|   Bachelor's degree|\n",
      "|Primary/elementar...|\n",
      "|       Undergraduate|\n",
      "| Professional degree|\n",
      "|     Master's degree|\n",
      "|    Associate degree|\n",
      "|    Secondary school|\n",
      "|     Doctoral degree|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see the actual values in the EducationLevel column\n",
    "dataframe_cleaned.select(\"EducationLevel\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the \"UndergradMajor\" feature, we have categorized the fields that are similar to each other into one category. These categories are added in a new column called \"Major\" and dropped \"UndergradMajor\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Hobbyist',\n",
       " 'ConvertedComp',\n",
       " 'Employment',\n",
       " 'OrgSize',\n",
       " 'YearsCodePro',\n",
       " 'Data scientist or machine learning specialist',\n",
       " 'Database administrator',\n",
       " 'Data or business analyst',\n",
       " 'Engineer, data',\n",
       " 'Region',\n",
       " 'Target',\n",
       " 'EducationLevel',\n",
       " 'Major']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned.select(\"UndergradMajor\").distinct().collect()\n",
    "dataframe_cleaned = dataframe_cleaned.withColumn(\n",
    "    \"Major\",\n",
    "    when(dataframe_cleaned[\"UndergradMajor\"].isin(\"Computer science\", \"Another engineering discipline\",\n",
    "        \"Mathematics or statistics\", \"Information systems\", \"Web development or web design\"), \"STEM\")\n",
    "    .when(dataframe_cleaned.UndergradMajor == \"Fine arts or performing arts\", \"Arts\")\n",
    "    .when(dataframe_cleaned.UndergradMajor.isin(\"Social science\", \"Natural science\", \"Health science\"), \"Science\")\n",
    "    .when(dataframe_cleaned.UndergradMajor == \"Humanities\", \"Humanities\")\n",
    "    .when(dataframe_cleaned.UndergradMajor == \"Business\", \"Business\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "# drop 'UndergradMajor' column from the predictors\n",
    "dataframe_cleaned = dataframe_cleaned.drop(dataframe_cleaned.UndergradMajor)\n",
    "dataframe_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     Major|\n",
      "+----------+\n",
      "|   Science|\n",
      "|Humanities|\n",
      "|     Other|\n",
      "|      STEM|\n",
      "|      Arts|\n",
      "|  Business|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's see the actual values in the Major column\n",
    "dataframe_cleaned.select(\"Major\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below part, we created a new column named \"OrganizationSize\" in the dataframe based on the values in the original \"OrgSize\" column using specific mapping rules, it's like a sequential order that we have created in our \"OrganizationSize\" column. Then we removed the \"OrgSize\" column from the set of predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Year',\n",
       " 'Hobbyist',\n",
       " 'ConvertedComp',\n",
       " 'Employment',\n",
       " 'YearsCodePro',\n",
       " 'Data scientist or machine learning specialist',\n",
       " 'Database administrator',\n",
       " 'Data or business analyst',\n",
       " 'Engineer, data',\n",
       " 'Region',\n",
       " 'Target',\n",
       " 'EducationLevel',\n",
       " 'Major',\n",
       " 'OrganizationSize']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned.select(\"OrgSize\").distinct().collect()\n",
    "dataframe_cleaned = dataframe_cleaned.withColumn(\n",
    "    \"OrganizationSize\",\n",
    "    when(dataframe_cleaned[\"OrgSize\"].isin(\"2 to 9 employees\", \"10 to 19 employees\"), \"11 - 50\")\n",
    "    .when(dataframe_cleaned.OrgSize == \"20 to 99 employees\", \"51 - 200\")\n",
    "    .when(dataframe_cleaned.OrgSize == \"100 to 499 employees\", \"201 - 500\")\n",
    "    .when(dataframe_cleaned.OrgSize == \"500 to 999 employees\", \"500 - 1000\")\n",
    "    .when(dataframe_cleaned.OrgSize.isin(\"1,000 to 4,999 employees\", \"5,000 to 9,999 employees\"), \"1001 - 10.000\")\n",
    "    .when(dataframe_cleaned.OrgSize == \"10,000 or more employees\", \"10.000+\")\n",
    "    .otherwise(\"Other\")\n",
    ")\n",
    "# drop 'OrgSize' column from the predictors\n",
    "dataframe_cleaned = dataframe_cleaned.drop(dataframe_cleaned.OrgSize)\n",
    "dataframe_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20230\n",
      "Region  [Row(Region='Europe'), Row(Region='Africa'), Row(Region='Oceania'), Row(Region='not found'), Row(Region='Asia'), Row(Region='America')]\n",
      "Major  [Row(Major='Science'), Row(Major='Humanities'), Row(Major='Other'), Row(Major='STEM'), Row(Major='Arts'), Row(Major='Business')]\n",
      "OrganizationSize  [Row(OrganizationSize='1001 - 10.000'), Row(OrganizationSize='201 - 500'), Row(OrganizationSize='Other'), Row(OrganizationSize='11 - 50'), Row(OrganizationSize='51 - 200'), Row(OrganizationSize='10.000+'), Row(OrganizationSize='500 - 1000')]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_cleaned.count())\n",
    "print(\"Region \", dataframe_cleaned.select(\"Region\").distinct().collect())\n",
    "print(\"Major \", dataframe_cleaned.select(\"Major\").distinct().collect())\n",
    "print(\"OrganizationSize \", dataframe_cleaned.select(\"OrganizationSize\").distinct().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, the \"Region\" column has \"not found\" as a value, \"Major\" and \"OrganizationSize\" columns have \"Other\" as values. These values mean in an indirect way \"null\" and will not add value to our analysis journey so we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18978"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned = dataframe_cleaned.filter(dataframe_cleaned.Region != \"not found\")\n",
    "dataframe_cleaned = dataframe_cleaned.filter(dataframe_cleaned.Major != \"Other\")\n",
    "dataframe_cleaned = dataframe_cleaned.filter(dataframe_cleaned.OrganizationSize != \"Other\")\n",
    "dataframe_cleaned.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Employment='Employed part-time'),\n",
       " Row(Employment='Employed full-time'),\n",
       " Row(Employment='Independent contractor, freelancer, or self-employed')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the \"Employment\" column for distinct values\n",
    "dataframe_cleaned.select(\"Employment\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Hobbyist='No'), Row(Hobbyist='Yes')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the \"Hobbyist\" column for distinct values\n",
    "dataframe_cleaned.select(\"Hobbyist\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Year': 2,\n",
       " 'Hobbyist': 2,\n",
       " 'ConvertedComp': 5551,\n",
       " 'Employment': 3,\n",
       " 'YearsCodePro': 30,\n",
       " 'Data scientist or machine learning specialist': 2,\n",
       " 'Database administrator': 2,\n",
       " 'Data or business analyst': 2,\n",
       " 'Engineer, data': 2,\n",
       " 'Region': 5,\n",
       " 'Target': 2,\n",
       " 'EducationLevel': 8,\n",
       " 'Major': 5,\n",
       " 'OrganizationSize': 6}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many distinct values each variable has - particularly important for non-numeric variables:\n",
    "unique_counts = {c:dataframe_cleaned.select(c).distinct().count() for c in dataframe_cleaned.columns}\n",
    "unique_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the different variables of our dataset and the first several records, we can identify that \"Hobbyist\", \"Employment\", \"Region\", \"EducationLevel\", \"Major\" and \"OrganizationSize\" are categorical variables. The rest are numerical or binary.\n",
    "\n",
    "Now let's look at how many samples fall into each target class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1|12515|\n",
      "|     0| 6463|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18978"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_cleaned.groupBy('Target').count().show()\n",
    "dataframe_cleaned.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that approximately 65% of employees are satisfied with their jobs and 35% of employees are not satisfied, meaning that our dataset is class imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Feature Selection\n",
    "\n",
    "#### Hypothesis Testing using Chi-square:\n",
    "\n",
    "As we have illustrated in the EDA phase our dataset consists of numerical and categorical variables, specifically six categorical variables. Usually, machine learning models perform better with numerical variables rather than categorical variables and some cannot deal with categorical variables directly unless preprocessing steps have been applied such as converting them into dummy variables or assigning a probability to each category for each feature. However, before applying any preprocessing steps, we would like to know which categorical features contribute to determining the target label and the Chi-Square test would help us to do this. \n",
    "\n",
    "The chi-square test is a statistical test used to determine which categorical features are the most important for predicting the target label by assuming two hypotheses: Null hypothesis and Alternative hypothesis. In our case, the null hypothesis in the chi-square test assumes that there is no association or independence between the categorical predictors and the target label, whereas the alternative hypothesis assumes that there is an association between the categorical variables and the target label. Chi-Square test provides many selection methods to decide which features to choose, We have chosen in our project the *P-value* selection method. The \"fpr\" parameter passed below to our chi-square test represents the p-value. The *\"fpr\"* chooses all features whose p-values are below a threshold (0.05), thus controlling the false positive rate of selection. In other words, if the p-value is below the predefined significance level (0.05), then we reject the null hypothesis and accept the alternative hypothesis. This indicates that the results are statistically significant.\n",
    "\n",
    "Below we will start by encoding our categorical features using StringIndexer() and then combine all these features into a single dense vector column called features using VectorAssembler(). Then we pass this vector column to ChiSqSelector() with fpr value equal to 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "|encoded_OrganizationSize|encoded_Region|encoded_Hobbyist|encoded_EducationLevel|encoded_Employment|encoded_Major|\n",
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "|0.0                     |3.0           |0.0             |2.0                   |0.0               |0.0          |\n",
      "|4.0                     |2.0           |0.0             |1.0                   |0.0               |0.0          |\n",
      "|0.0                     |1.0           |0.0             |1.0                   |0.0               |0.0          |\n",
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------+\n",
      "|selectedFeatures         |\n",
      "+-------------------------+\n",
      "|(6,[1,3],[3.0,2.0])      |\n",
      "|[4.0,2.0,0.0,1.0,0.0,0.0]|\n",
      "|(6,[1,3],[1.0,1.0])      |\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your categorical features\n",
    "categorical_features = ['OrganizationSize','Region', 'Hobbyist', 'EducationLevel', 'Employment', 'Major']\n",
    "\n",
    "# Initialize a StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=categorical_features, outputCols=[\"encoded_\" + col for col in categorical_features])\n",
    "\n",
    "# Fit the StringIndexer\n",
    "model = stringIndexer.fit(dataframe_cleaned)\n",
    "\n",
    "# Transform the data\n",
    "dataframe_with_encoded_features = model.transform(dataframe_cleaned)\n",
    "\n",
    "# Create a VectorAssembler to assemble the features into a DenseVector\n",
    "encoded_features = [\"encoded_OrganizationSize\", \"encoded_Region\", \"encoded_Hobbyist\", \"encoded_EducationLevel\", \"encoded_Employment\", \"encoded_Major\"]\n",
    "\n",
    "# the result is in \"features\" columns\n",
    "assembler = VectorAssembler(inputCols=encoded_features, outputCol=\"features\")\n",
    "\n",
    "selector = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"Target\", fpr= 0.05)\n",
    "\n",
    "# Create a pipeline consists of assembler followed by the ChiSqSelector\n",
    "pipeline = Pipeline(stages=[assembler, selector])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(dataframe_with_encoded_features)\n",
    "\n",
    "# Transform the data\n",
    "result = model.transform(dataframe_with_encoded_features)\n",
    "\n",
    "dataframe_with_encoded_features[encoded_features].show(n=3, truncate=False)\n",
    "\n",
    "# The selectedFeatures column now contains the most important features\n",
    "selected_features = result.select(\"selectedFeatures\")\n",
    "selected_features.show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"selectedFeatures\" column now contains the most important features. Below we will manually map the selected features to the original encoded variables to know which top 3 features we will proceed with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoded_Region', 'encoded_EducationLevel', 'encoded_OrganizationSize']\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to map selected feature indices to variable names\n",
    "feature_mapping = {\n",
    "    0: \"encoded_OrganizationSize\",\n",
    "    1: \"encoded_Region\",\n",
    "    2: \"encoded_Hobbyist\",\n",
    "    3: \"encoded_EducationLevel\",\n",
    "    4: \"encoded_Employment\",\n",
    "    5: \"encoded_Major\"\n",
    "}\n",
    "\n",
    "# Extract the selected feature indices from the DataFrame\n",
    "selected_feature_indices = result.select(\"selectedFeatures\").collect()[0][0].toArray()\n",
    "\n",
    "# Sort the indices based on their values (Chi-Square statistics)\n",
    "sorted_indices = sorted(range(len(selected_feature_indices)), key=lambda i: selected_feature_indices[i], reverse=True)\n",
    "\n",
    "# Select the top 3 feature indices\n",
    "top_3_indices = sorted_indices[:3]\n",
    "\n",
    "# Map selected feature indices to variable names\n",
    "selected_features = [feature_mapping[i] for i in top_3_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the Chi-Square test results, the categorical features that have been determined to be the most predictive of the target label are: 'encoded_Region', 'encoded_EducationLevel'and 'encoded_OrganizationSize'. They have the highest Chi-Square statistics among the other encoded categorical features. We will proceed with them in our analysis and drop the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Hobbyist)\n",
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Employment)\n",
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Major)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and their corresponding values\n",
    "dataframe_with_encoded_features.show(n=1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Planned Analysis\n",
    "\n",
    "\n",
    "#### 1.5.1 Pre-processing\n",
    "\n",
    "We have demonstrated in the EDA section when we used the \"group by\" query to group our dataset by target, we found that 12515 samples belong to class \"1\" and 6463 samples belong to class \"0\" thus we have an imbalanced target variable dataset. Since our class label distribution is skewed, leading to biased results, we balanced our data to have more accurate and precise results from our models. We have many techniques to balance our dataset but the most popular one is *Data Sampling*. Data sampling can be done using oversampling techniques or undersampling techniques or using both together, we have chosen to do oversampling to make use of HDFS and Apache spark characteristics in our big data analysis journey. Oversampling is done by duplicating the number of data points that belong to the minority class, which is in our case class 0, to match the the number of the majority class samples (class 1). \n",
    "\n",
    "The second pre-processing step is One-hot encoding, which is a technique used to convert categorical variables to a vector form that our machine-learning model can understand. In other words, each category will be a binary feature indicating whether or not the sample belongs to that category.\n",
    "\n",
    "\n",
    "The third and the last step in the pre-processing phase is Scaling numerical variables. Now our data is partially ready to be passed to the Spark ML models, as they are presented in numerical values but in different ranges. Some have large values and some have small values so we can say that our data is *heterogeneous* which means one feature can be between 0-1 range and another one can be between 50-100 range. Feeding our Spark ML models with a wide range of values per feature will affect some models' performance so we will scale each feature by letting each one vary roughly within the same range, thus our data will be *homogenous*. Scaling is the process of transforming the data within a specific range, this range is usually between 0 and 1. We will apply the Min-Max Scaling technique, which is most of the time the go-to technique to scale the data. It scales the data within a specified range where the minimum value is 0 and the maximum value is 1, [0,1] is the default range of the MinMaxScaler, these values could be changed but we will leave them as they are.\n",
    "\n",
    "\n",
    "#### 1.5.2 Machine learning models\n",
    "\n",
    "We chose to use the Spark ML library (dataframe/dataset-based) rather than the MLlib (RDD-based) library due to its ease of use by providing high levels of APIs, seamlessly integrating with dataframe and SQL API, adding the models to a pipeline - MLlib models cannot be added to a pipeline and many more rich features. \n",
    "\n",
    "Spark ML library contains many models for classification, we have used three of them as it is unknown ahead of time which model type may be most suitable ahead of time. We started with Random Forest followed by Naive Bayes and Logistic Regression. We have used L1 regularization in the Logistic Regression model to reduce the risk of overfitting and this is due to its ability to perform in-model feature selection. In other words, L1 regularization excludes the features that contribute little to the model's predictive power by setting the weights of less important features to zero. In addition, it becomes more beneficial after applying one hot encoded, as many binary columns are the result, so it excludes unnecessary binary columns that might not significantly contribute to the model's performance.\n",
    "\n",
    "We utilized a random search approach for tuning the three models and a hold-out validation technique to validate our models which is the process of splitting the dataset into three subsets: the training set, the validation set, and the testing set. The training set is used to train our machine learning model, the validation set is used for tuning our model's hyperparameters and the test set is to evaluate our model's performance. We set aside 20% of training and validation data for evaluating models' hyperparameters tuning. The holdout validation technique is the best way to consider when you have a large dataset so you can split it into three sets. \n",
    "\n",
    "Finally, we trained the best-performing model on the training data using the best set of parameters we reached and then calculated the accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation\n",
    "\n",
    "#### 2.1 Pre-processing:\n",
    "\n",
    "##### 2.1.1 Balancing Dataset using Oversampling\n",
    "\n",
    "Below we are oversampling the minority class by calculating an oversampling fraction for the \"*0*\" target label. This fraction calculates the count of instances for target label '1' divided by an arbitrary factor (2.12), we reached this factor after many trials, and then divided by the count of instances for target label '0'. In the end, an oversampled dataframe is created with additional instances with a target label of '0'. Thus the class label distribution is balanced by increasing the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_0  6463\n",
      "count_1  12515\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1|12515|\n",
      "|     0|12500|\n",
      "+------+-----+\n",
      "\n",
      "new dataset size:  25015\n",
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = dataframe_with_encoded_features.groupBy('Target').count()\n",
    "\n",
    "\n",
    "# Get the count for the 0 and 1 target labels\n",
    "count_0 = counts.filter(counts['Target'] == 0).select('count').collect()[0][0]\n",
    "count_1 = counts.filter(counts['Target'] == 1).select('count').collect()[0][0]\n",
    "print(\"count_0 \", count_0)\n",
    "print(\"count_1 \", count_1)\n",
    "\n",
    "# Calculate the oversampling fraction for the 0 target label\n",
    "oversampling_fraction_0 = (count_1/2.12) / count_0\n",
    "\n",
    "# Create a DataFrame with additional rows for the 0 target label\n",
    "oversampled_df = dataframe_with_encoded_features.unionAll(\n",
    "    dataframe_with_encoded_features.filter(dataframe_with_encoded_features['Target'] == 0)\n",
    "    .sample(True, oversampling_fraction_0 , seed=200)\n",
    "    .withColumn('Target', lit(0))\n",
    ")\n",
    "\n",
    "# We can now see our classes are approximately balanced:\n",
    "oversampled_df.groupBy('Target').count().show()\n",
    "# new dataset size:\n",
    "print(\"new dataset size: \",oversampled_df.count())\n",
    "# Show the oversampled DataFrame\n",
    "oversampled_df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Feature Engineering:\n",
    "\n",
    "In this section, we will apply one-hot encoding on categorical features and scale the numerical features by iterating through each column that requires processing within a for loop and running a pipeline that consists of the required stages.\n",
    "\n",
    "\n",
    "#### 2.1.2.1 One-hot Encoding for Categorical Variables\n",
    "\n",
    "The categorical features resulting from Chi-Square statistical test were: 'encoded_Region', 'encoded_EducationLevel', and 'encoded_OrganizationSize'. We can consider 'encoded_EducationLevel' as an ordinal variable since no one, for instance, can have a Bachelor's degree without completing high school. Also, we can consider the 'encoded_OrganizationSize' an ordinal variable since an organization with size '11 - 50' is smaller than an organization with size '51 - 200', so applying one hot encoding on ordinal variables would not make sense. Therefore, we will apply one-hot encoding on the 'encoded_Region' variable only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      " Region_index                                  | 3.0                \n",
      " Region_vector                                 | (4,[3],[1.0])      \n",
      "-RECORD 1-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 13293.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 10                 \n",
      " Data scientist or machine learning specialist | 1                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 1                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Asia               \n",
      " Target                                        | 0                  \n",
      " EducationLevel                                | Master's degree    \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 10.000+            \n",
      " encoded_EducationLevel                        | 1.0                \n",
      " encoded_OrganizationSize                      | 4.0                \n",
      " encoded_Region                                | 2.0                \n",
      " Region_index                                  | 2.0                \n",
      " Region_vector                                 | (4,[2],[1.0])      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the list of columns to be one-hot encoded\n",
    "columns_to_encode = [\"Region\"]   \n",
    "\n",
    "# Create empty lists to hold the StringIndexers and Encoders\n",
    "string_indexers = []\n",
    "encoders = []\n",
    "\n",
    "# Loop through each column and create a StringIndexer and an Encoder for it\n",
    "for col in columns_to_encode:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vector\")\n",
    "    string_indexers.append(indexer)\n",
    "    encoders.append(encoder)\n",
    "\n",
    "# Create a pipeline with StringIndexers and Encoders\n",
    "pipeline = Pipeline(stages=string_indexers + encoders)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = pipeline.fit(oversampled_df)\n",
    "encoded_df = model.transform(oversampled_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "encoded_df.show(n=2,vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have in our dataset 'encoded_Region' and 'Region_index' where both of them represent the index of the 'Region' category so we will drop one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Hobbyist: string (nullable = true)\n",
      " |-- ConvertedComp: double (nullable = true)\n",
      " |-- Employment: string (nullable = false)\n",
      " |-- YearsCodePro: integer (nullable = true)\n",
      " |-- Data scientist or machine learning specialist: integer (nullable = true)\n",
      " |-- Database administrator: integer (nullable = true)\n",
      " |-- Data or business analyst: integer (nullable = true)\n",
      " |-- Engineer, data: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- Target: integer (nullable = false)\n",
      " |-- EducationLevel: string (nullable = false)\n",
      " |-- Major: string (nullable = false)\n",
      " |-- OrganizationSize: string (nullable = false)\n",
      " |-- encoded_EducationLevel: double (nullable = false)\n",
      " |-- encoded_OrganizationSize: double (nullable = false)\n",
      " |-- Region_index: double (nullable = false)\n",
      " |-- Region_vector: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"encoded_Region\"]\n",
    "encoded_df=encoded_df.drop(*columns_to_drop)\n",
    "encoded_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.2 Scaling Numerical Variables\n",
    "\n",
    "There are only four numerical variables that have a different range of values: \"ConvertedComp\", \"YearsCodePro\", \"encoded_EducationLevel\", \"encoded_OrganizationSize\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " Year                                          | 2019                  \n",
      " Hobbyist                                      | Yes                   \n",
      " ConvertedComp                                 | 95179.0               \n",
      " Employment                                    | Employed full-time    \n",
      " YearsCodePro                                  | 4                     \n",
      " Data scientist or machine learning specialist | 0                     \n",
      " Database administrator                        | 1                     \n",
      " Data or business analyst                      | 0                     \n",
      " Engineer, data                                | 0                     \n",
      " Region                                        | Oceania               \n",
      " Target                                        | 1                     \n",
      " EducationLevel                                | Undergraduate         \n",
      " Major                                         | STEM                  \n",
      " OrganizationSize                              | 11 - 50               \n",
      " encoded_EducationLevel                        | 2.0                   \n",
      " encoded_OrganizationSize                      | 0.0                   \n",
      " Region_index                                  | 3.0                   \n",
      " Region_vector                                 | (4,[3],[1.0])         \n",
      " ConvertedComp_vector                          | [95179.0]             \n",
      " YearsCodePro_vector                           | [4.0]                 \n",
      " encoded_EducationLevel_vector                 | [2.0]                 \n",
      " encoded_OrganizationSize_vector               | [0.0]                 \n",
      " ConvertedComp_scaled                          | [0.31900923396624825] \n",
      " YearsCodePro_scaled                           | [0.10344827586206896] \n",
      " encoded_EducationLevel_scaled                 | [0.2857142857142857]  \n",
      " encoded_OrganizationSize_scaled               | [0.0]                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to scale\n",
    "columns_to_scale = [\"ConvertedComp\", \"YearsCodePro\", \"encoded_EducationLevel\", \"encoded_OrganizationSize\"]\n",
    "\n",
    "assemblers = []\n",
    "scalers = []\n",
    "\n",
    "for col in columns_to_scale:\n",
    "    # Assemble the features into a vector\n",
    "    assembler = VectorAssembler(inputCols=[col], outputCol=col + \"_vector\")\n",
    "    scaler = MinMaxScaler(inputCol=col + \"_vector\", outputCol=col + \"_scaled\")\n",
    "    assemblers.append(assembler)\n",
    "    scalers.append(scaler)\n",
    "\n",
    "# Create pipeline\n",
    "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = scaling_pipeline.fit(encoded_df)\n",
    "scaled_df = model.transform(encoded_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "scaled_df.show(n=1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to proceed only with the following variables: one-hot encoded, scaled, our target label, and the binary variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " Data scientist or machine learning specialist | 0                     \n",
      " Database administrator                        | 1                     \n",
      " Data or business analyst                      | 0                     \n",
      " Engineer, data                                | 0                     \n",
      " Target                                        | 1                     \n",
      " Region_vector                                 | (4,[3],[1.0])         \n",
      " ConvertedComp_scaled                          | [0.31900923396624825] \n",
      " YearsCodePro_scaled                           | [0.10344827586206896] \n",
      " encoded_EducationLevel_scaled                 | [0.2857142857142857]  \n",
      " encoded_OrganizationSize_scaled               | [0.0]                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"Year\", \"Hobbyist\", \"ConvertedComp\", \"Employment\", \"YearsCodePro\", \"Region\", \"EducationLevel\", \n",
    "                   \"Major\", \"OrganizationSize\", \"encoded_EducationLevel\", \"encoded_EducationLevel_vector\", \"encoded_Region\",\n",
    "                   \"encoded_OrganizationSize_vector\", \"encoded_OrganizationSize\", \"Region_index\",\"ConvertedComp_vector\", \n",
    "                   \"YearsCodePro_vector\"]\n",
    "\n",
    "spark_df = scaled_df.drop(*columns_to_drop)\n",
    "spark_df.show(n=1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage our dataset is ready to split into train, validation, and test sets, however, we still need to modify our features to make more accurate predictions and this is done by assembling individual variables into a single feature vector, which is done by using Spark‚Äôs VectorAssembler. The VectorAssembler takes a list of columns as an input and combines them into a single vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------\n",
      " features | (12,[1,7,8,9,10],[1.0,1.0,0.31900923396624825,0.10344827586206896,0.2857142857142857])            \n",
      " Target   | 1                                                                                                 \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------\n",
      " features | [1.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,0.044550954399959784,0.3103448275862069,0.14285714285714285,0.8] \n",
      " Target   | 0                                                                                                 \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------\n",
      " features | [0.0,1.0,1.0,1.0,0.0,1.0,0.0,0.0,0.30165071810427174,0.24137931034482757,0.14285714285714285,0.0] \n",
      " Target   | 1                                                                                                 \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a VectorAssembler to be passed to our models\n",
    "feature_cols = [c for c in spark_df.columns if c != 'Target']\n",
    "assembler = VectorAssembler(inputCols=feature_cols , outputCol=\"features\")\n",
    "\n",
    "# Use transform to assemble the columns\n",
    "assembled_df = assembler.transform(spark_df)  \n",
    "\n",
    "selected_columns = [\"features\", \"Target\"]\n",
    "assembled_df = assembled_df.select(selected_columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "assembled_df.show(n=3, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Predictive Models:\n",
    " \n",
    "#### 2.2.1 Holdout Validation:\n",
    "\n",
    "In this section, we will start by splitting the above results into three sets: training set, validation set, and testing set. The holdout validation technique is the best way to consider when you have a large dataset so you can split it into three sets: the training set represents 80% of the whole dataset, the validation set represents 20% of the training set, and the testing set represents 20% of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------\n",
      " features | (12,[0,1,2,3,5,8],[1.0,1.0,1.0,1.0,1.0,0.006700072061805567]) \n",
      " Target   | 1                                                             \n",
      "only showing top 1 row\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 7988|\n",
      "|     0| 8140|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 2006|\n",
      "|     0| 1929|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 2521|\n",
      "|     0| 2431|\n",
      "+------+-----+\n",
      "\n",
      "16128\n",
      "3935\n",
      "4952\n"
     ]
    }
   ],
   "source": [
    "# holdout validation\n",
    "(train, test) = assembled_df.randomSplit([0.8, 0.2], seed = 5)\n",
    "(train_df, val_df) = train.randomSplit([0.8, 0.2], seed = 5)\n",
    "\n",
    "# Show a sample of the training set\n",
    "train.show(n=1, vertical=True, truncate=False)\n",
    "\n",
    "# Show the total number of samples for each class label per set\n",
    "train_df.groupBy('Target').count().show()\n",
    "val_df.groupBy('Target').count().show()\n",
    "test.groupBy('Target').count().show()\n",
    "\n",
    "# Show total number of samples per set\n",
    "print(train_df.count())\n",
    "print(val_df.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Random Forest:\n",
    "\n",
    "\n",
    "We chose Random Forests because it does internal feature selection so that would emphasize to us that the features we have selected are the most contributing features to the class label. Random Forests is considered a supervised learning technique. One of its biggest advantages is that it can be used for regression and classification as well.\n",
    "\n",
    "Random Forests works by comprising the maximum number of decision trees so it ends with a strong and accurate forest. Random Forests are considered an ensemble method that is based on bagging or bootstrapping + aggregating. How it internally works is that it builds multiple decision trees on bootstrapped subsamples of the data. Afterward, a subset of the columns also is chosen in each tree. The tree is built and the response variable with the highest decision with the most votes is chosen. By aggregating or choosing the number of columns arbitrary feature selection comes in included implicitly. What makes the random forest classifier our first chosen algorithm, regardless of the pre-processing steps made on the dataset, to detect whether this employee is satisfied or not is the following:\n",
    "\n",
    "* Random forest produces the output with high accuracy because it does not depend on the output of one decision tree only but uses the voting technique, meaning most class labels dominant.\n",
    "\n",
    "* Random forest avoids overfitting, if one decision tree is biased because of missing values or outliers, the other decision tree will balance this by ensuring the diversity within the samples so that each tree is different from the other in terms of features and data due to the bagging method thus the trees are not dependent on each other hence less prone to overfitting\n",
    "\n",
    "* Random forest can handle missing values by substituting these null values with the mean or the median value per feature so this corrects the bias problem\n",
    "\n",
    "* Random Forest provides feature selection which shows the features that mostly contribute to the classification/prediction by calculating the importance score of the feature and the features with low importance could be removed to improve the overall model performance. And this step can be made in the future to emphasize our chi-square test results. \n",
    "\n",
    "* Moreover, random forests can handle huge amounts of attributes with high accuracy and with minimum bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start1 = time.time()\n",
    "\n",
    "rf_tuning1 = pd.DataFrame(columns=['numTrees', 'maxDepth', 'maxBins', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 4):\n",
    "\n",
    "    # Randomly select values for hyperparameters from a range\n",
    "    trees1 = random.randrange(5, 201, 5)\n",
    "    depth1 = random.randrange(2, 21, 2)\n",
    "    bins1 = random.randrange(10, 61, 5)\n",
    "\n",
    "    # Create a RandomForestClassifier instance\n",
    "    randomForestClassifier1 = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features\",\n",
    "                                                     numTrees=trees1, maxDepth=depth1, maxBins=bins1)\n",
    "\n",
    "    randomForestModel1 = randomForestClassifier1.fit(train_df)\n",
    "\n",
    "    # // run model with validation data set to get predictions\n",
    "    predictions1 = randomForestModel1.transform(val_df)\n",
    "    predictions1.select(\"Target\", \"prediction\").show()\n",
    "    \n",
    "end1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6348490117984062\n",
      "Validation Error = 0.36515098820159375\n"
     ]
    }
   ],
   "source": [
    "evaluator1 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "accuracy1 = evaluator1.evaluate(predictions1)\n",
    "print(\"Accuracy = %s\" % (accuracy1))\n",
    "print(\"Validation Error = %s\" % (1.0 - accuracy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) for random forest classifier tuning: 7.8935590545336405\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) for random forest classifier tuning:', (end1-start1)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different values for our hyperparameters while doing random search to tune our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start2 = time.time()\n",
    "\n",
    "rf_tuning2 = pd.DataFrame(columns=['numTrees', 'maxDepth', 'maxBins', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 5):\n",
    "\n",
    "    # Randomly select values for hyperparameters from a range\n",
    "    trees2 = random.randrange(5, 101, 5)\n",
    "    depth2 = random.randrange(2, 21, 2)\n",
    "    bins2 = random.randrange(10, 61, 5)\n",
    "\n",
    "    # Create a RandomForestClassifier instance\n",
    "    randomForestClassifier2 = RandomForestClassifier(labelCol=\"Target\",\n",
    "    featuresCol=\"features\", numTrees=trees2, maxDepth=depth2, maxBins=bins2)\n",
    "\n",
    "    randomForestModel2 = randomForestClassifier2.fit(train_df)\n",
    "\n",
    "    # // run model with test data set to get predictions\n",
    "    predictions2 = randomForestModel2.transform(val_df)\n",
    "    predictions2.select(\"Target\", \"prediction\").show()\n",
    "\n",
    "end2 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6823247426479375\n",
      "Validation Error = 0.3176752573520625\n"
     ]
    }
   ],
   "source": [
    "evaluator2 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "accuracy2 = evaluator2.evaluate(predictions2)\n",
    "print(\"Accuracy = %s\" % (accuracy2))\n",
    "print(\"Validation Error = %s\" % (1.0 - accuracy2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) for random forest classifier tuning: 1.3447409868240356\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) for random forest classifier tuning:', (end2-start2)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another trial\n",
    "start3 = time.time()\n",
    "\n",
    "rf_tuning3 = pd.DataFrame(columns=['numTrees', 'maxDepth', 'maxBins', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 1):\n",
    "\n",
    "    # Randomly select values for hyperparameters from a range\n",
    "    trees3 = random.randrange(5, 61, 5)\n",
    "    depth3 = random.randrange(2, 21, 2)\n",
    "    bins3 = random.randrange(10, 61, 5)\n",
    "\n",
    "    # Create a RandomForestClassifier instance\n",
    "    randomForestClassifier3 = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features\", numTrees=trees3,\n",
    "                                                     maxDepth=depth3, maxBins=bins3)\n",
    "\n",
    "    randomForestModel3 = randomForestClassifier3.fit(train_df)\n",
    "\n",
    "    # // run model with test data set to get predictions\n",
    "    predictions3 = randomForestModel3.transform(val_df)\n",
    "    predictions3.select(\"Target\", \"prediction\").show()\n",
    "\n",
    "end3 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) for random forest classifier tuning: 0.707856031258901\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) for random forest classifier tuning:', (end3-start3)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6999355515792745\n",
      "Validation Error = 0.30006444842072555\n"
     ]
    }
   ],
   "source": [
    "evaluator3 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "accuracy3 = evaluator3.evaluate(predictions3)\n",
    "print(\"Accuracy = %s\" % (accuracy3))\n",
    "print(\"Validation Error = %s\" % (1.0 - accuracy3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider the last trial as the best resulting model since it has the highest accuracy and the lowest time taken to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Naive Bayes:\n",
    "\n",
    "The Na√Øve Bayes classifier is a conditional probabilistic classifier that assumes that all our dataset features are independent of each other and it's called *na√Øve* because this is a na√Øve assumption and not applied to real-life scenarios, nevertheless surprisingly it performs well and leads to convincing results. It's a probabilistic classifier because it's based on the Bayes theorem that calculates the posterior probability and prior probability while building the frequency and likelihood tables.\n",
    "\n",
    "In Na√Øves Bayes algorithm, we compute the frequency and the probability tables for each attribute per class label then substitute these probability values in the Na√Øve Bayesian equation to calculate the posterior probability per class label ( **P(y|X)** ) and the class label with the highest posterior value is considered as the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_start = time.time()\n",
    "\n",
    "# Create a NaiveBayes classifier\n",
    "nb = NaiveBayes(labelCol=\"Target\", featuresCol=\"features\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb_model = nb.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "nb_predictions = nb_model.transform(val_df)\n",
    "nb_predictions.select(\"Target\", \"prediction\").show()\n",
    "\n",
    "nb_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) to fit Naive Bayes model and make predictions: 0.04904579718907674\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) to fit Naive Bayes model and make predictions:', (nb_end-nb_start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5532100114427616\n",
      "Validation Error = 0.4467899885572384\n"
     ]
    }
   ],
   "source": [
    "nb_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "nb_accuracy = nb_evaluator.evaluate(nb_predictions)\n",
    "print(\"Accuracy = %s\" % (nb_accuracy))\n",
    "print(\"Validation Error = %s\" % (1.0 - nb_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Logistic Regression:\n",
    "\n",
    "Since our main target is to predict whether the employee is satisfied or not thus we used the Logistic regression model to help us in this prediction.\n",
    "\n",
    "Logistic Regression is the modeling approach that can be used to describe the relationship between several input variables/predictors and a class label which is either 0 or 1. In more detail, logistic regression predicts the probability of an outcome that can only have two values and then assigns to it a class label. If the probability is less than 0.5 then class 0 is assigned else class 1 is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-058fface11ce>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning = lr_tuning.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-058fface11ce>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning = lr_tuning.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-058fface11ce>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning = lr_tuning.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-058fface11ce>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning = lr_tuning.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-75-058fface11ce>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning = lr_tuning.append({'regParam': reg, 'iterations': iters,\n"
     ]
    }
   ],
   "source": [
    "lr_start = time.time()\n",
    "\n",
    "lr_tuning = pd.DataFrame(columns=['regParam', 'iterations', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    # Create a LogisticRegression classifier\n",
    "    reg = random.uniform(0, 0.05)\n",
    "    iters = random.randrange(10, 101, 5)\n",
    "\n",
    "    # Create a LogisticRegression instance with the selected hyperparameters\n",
    "    lr = LogisticRegression(labelCol=\"Target\", featuresCol=\"features\", regParam=reg, maxIter=iters, elasticNetParam=1.0,\n",
    "                            family=\"auto\")\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    lr_model = lr.fit(train_df)\n",
    "    \n",
    "    # Make predictions on the validation data\n",
    "    lr_predictions = lr_model.transform(val_df)\n",
    "    lr_predictions.select(\"Target\", \"prediction\").show()\n",
    "    \n",
    "lr_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) to fit Logistic Regression model and make predictions: 0.7262279152870178\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) to fit Logistic Regression model and make predictions:', (lr_end-lr_start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5632500926110942\n",
      "Validation Error = 0.43674990738890584\n"
     ]
    }
   ],
   "source": [
    "# Define the MulticlassClassificationEvaluator\n",
    "lr_evaluator = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "lr_accuracy = lr_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(\"Accuracy = %s\" % (lr_accuracy))\n",
    "print(\"Validation Error = %s\" % (1.0 - lr_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.2.5 Considering All The Categorical Features:\n",
    "\n",
    "As the accuracy of models so far has been limited, we decided to see if the accuracy of models could be improved by considering all the categorical variables during our analysis, not only the features resulting from the chi-square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "|encoded_OrganizationSize|encoded_Region|encoded_Hobbyist|encoded_EducationLevel|encoded_Employment|encoded_Major|\n",
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "|0.0                     |3.0           |0.0             |2.0                   |0.0               |0.0          |\n",
      "|4.0                     |2.0           |0.0             |1.0                   |0.0               |0.0          |\n",
      "|0.0                     |1.0           |0.0             |1.0                   |0.0               |0.0          |\n",
      "+------------------------+--------------+----------------+----------------------+------------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------------+\n",
      "|selectedFeatures         |\n",
      "+-------------------------+\n",
      "|(6,[1,3],[3.0,2.0])      |\n",
      "|[4.0,2.0,0.0,1.0,0.0,0.0]|\n",
      "|(6,[1,3],[1.0,1.0])      |\n",
      "+-------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define your categorical features\n",
    "categorical_features = ['OrganizationSize','Region', 'Hobbyist', 'EducationLevel', 'Employment', 'Major']\n",
    "\n",
    "# Initialize a StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=categorical_features, outputCols=[\"encoded_\" + col for col in categorical_features])\n",
    "\n",
    "# Fit the StringIndexer\n",
    "model = stringIndexer.fit(dataframe_cleaned)\n",
    "\n",
    "# Transform the data\n",
    "dataframe_with_encoded_features = model.transform(dataframe_cleaned)\n",
    "\n",
    "# Create a VectorAssembler to assemble the features into a DenseVector\n",
    "encoded_features = [\"encoded_OrganizationSize\", \"encoded_Region\", \"encoded_Hobbyist\", \"encoded_EducationLevel\", \"encoded_Employment\", \"encoded_Major\"]\n",
    "\n",
    "# the result is in \"features\" columns\n",
    "assembler = VectorAssembler(inputCols=encoded_features, outputCol=\"features\")\n",
    "\n",
    "selector = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"Target\", fpr= 0.05)\n",
    "\n",
    "# Create a pipeline consists of assembler followed by the ChiSqSelector\n",
    "pipeline = Pipeline(stages=[assembler, selector])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(dataframe_with_encoded_features)\n",
    "\n",
    "# Transform the data\n",
    "result = model.transform(dataframe_with_encoded_features)\n",
    "\n",
    "dataframe_with_encoded_features[encoded_features].show(n=3, truncate=False)\n",
    "\n",
    "# The selectedFeatures column now contains the most important features\n",
    "selected_features = result.select(\"selectedFeatures\")\n",
    "selected_features.show(n=3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoded_Region', 'encoded_EducationLevel', 'encoded_OrganizationSize', 'encoded_Hobbyist', 'encoded_Employment', 'encoded_Major']\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary to map selected feature indices to variable names\n",
    "feature_mapping = {\n",
    "    0: \"encoded_OrganizationSize\",\n",
    "    1: \"encoded_Region\",\n",
    "    2: \"encoded_Hobbyist\",\n",
    "    3: \"encoded_EducationLevel\",\n",
    "    4: \"encoded_Employment\",\n",
    "    5: \"encoded_Major\"\n",
    "}\n",
    "\n",
    "# Extract the selected feature indices from the DataFrame\n",
    "selected_feature_indices = result.select(\"selectedFeatures\").collect()[0][0].toArray()\n",
    "\n",
    "# Sort the indices based on their values (Chi-Square statistics)\n",
    "sorted_indices = sorted(range(len(selected_feature_indices)), key=lambda i: selected_feature_indices[i], reverse=True)\n",
    "\n",
    "# Select the top 3 feature indices\n",
    "top_3_indices = sorted_indices[:]\n",
    "\n",
    "# Map selected feature indices to variable names\n",
    "selected_features = [feature_mapping[i] for i in top_3_indices]\n",
    "\n",
    "# Print the selected feature names\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Hobbyist                              | 0.0                \n",
      " encoded_Major                                 | 0.0                \n",
      " encoded_Employment                            | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and their corresponding values\n",
    "dataframe_with_encoded_features.show(n=1, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_0  6463\n",
      "count_1  12515\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1|12515|\n",
      "|     0|12500|\n",
      "+------+-----+\n",
      "\n",
      "new dataset size:  25015\n",
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Hobbyist                              | 0.0                \n",
      " encoded_Major                                 | 0.0                \n",
      " encoded_Employment                            | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counts = dataframe_with_encoded_features.groupBy('Target').count()\n",
    "\n",
    "\n",
    "# Get the count for the 0 and 1 target labels\n",
    "count_0 = counts.filter(counts['Target'] == 0).select('count').collect()[0][0]\n",
    "count_1 = counts.filter(counts['Target'] == 1).select('count').collect()[0][0]\n",
    "print(\"count_0 \", count_0)\n",
    "print(\"count_1 \", count_1)\n",
    "\n",
    "# Calculate the oversampling fraction for the 0 target label\n",
    "oversampling_fraction_0 = (count_1/2.12) / count_0\n",
    "\n",
    "# Create a DataFrame with additional rows for the 0 target label\n",
    "oversampled_df = dataframe_with_encoded_features.unionAll(\n",
    "    dataframe_with_encoded_features.filter(dataframe_with_encoded_features['Target'] == 0)\n",
    "    .sample(True, oversampling_fraction_0 , seed=200)\n",
    "    .withColumn('Target', lit(0))\n",
    ")\n",
    "\n",
    "# We can now see our classes are approximately balanced:\n",
    "oversampled_df.groupBy('Target').count().show()\n",
    "# new dataset size:\n",
    "print(\"new dataset size: \",oversampled_df.count())\n",
    "# Show the oversampled DataFrame\n",
    "oversampled_df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need to encode the \"Hobbyist\" feature as it's a binary variable. Moreover, as we mentioned above, we can consider the \"EducationLevel\" and \"OrganizationSize\" ordinal variables so encoding them does not make sense. Therefore, we will encode only the features that have multiple categories which are: \"Region\", \"Major\", and \"Employment\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 95179.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 4                  \n",
      " Data scientist or machine learning specialist | 0                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 0                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Oceania            \n",
      " Target                                        | 1                  \n",
      " EducationLevel                                | Undergraduate      \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 11 - 50            \n",
      " encoded_EducationLevel                        | 2.0                \n",
      " encoded_OrganizationSize                      | 0.0                \n",
      " encoded_Hobbyist                              | 0.0                \n",
      " encoded_Major                                 | 0.0                \n",
      " encoded_Employment                            | 0.0                \n",
      " encoded_Region                                | 3.0                \n",
      " Region_index                                  | 3.0                \n",
      " Major_index                                   | 0.0                \n",
      " Employment_index                              | 0.0                \n",
      " Region_vector                                 | (4,[3],[1.0])      \n",
      " Major_vector                                  | (4,[0],[1.0])      \n",
      " Employment_vector                             | (2,[0],[1.0])      \n",
      "-RECORD 1-----------------------------------------------------------\n",
      " Year                                          | 2019               \n",
      " Hobbyist                                      | Yes                \n",
      " ConvertedComp                                 | 13293.0            \n",
      " Employment                                    | Employed full-time \n",
      " YearsCodePro                                  | 10                 \n",
      " Data scientist or machine learning specialist | 1                  \n",
      " Database administrator                        | 1                  \n",
      " Data or business analyst                      | 1                  \n",
      " Engineer, data                                | 0                  \n",
      " Region                                        | Asia               \n",
      " Target                                        | 0                  \n",
      " EducationLevel                                | Master's degree    \n",
      " Major                                         | STEM               \n",
      " OrganizationSize                              | 10.000+            \n",
      " encoded_EducationLevel                        | 1.0                \n",
      " encoded_OrganizationSize                      | 4.0                \n",
      " encoded_Hobbyist                              | 0.0                \n",
      " encoded_Major                                 | 0.0                \n",
      " encoded_Employment                            | 0.0                \n",
      " encoded_Region                                | 2.0                \n",
      " Region_index                                  | 2.0                \n",
      " Major_index                                   | 0.0                \n",
      " Employment_index                              | 0.0                \n",
      " Region_vector                                 | (4,[2],[1.0])      \n",
      " Major_vector                                  | (4,[0],[1.0])      \n",
      " Employment_vector                             | (2,[0],[1.0])      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the list of columns to be one-hot encoded\n",
    "columns_to_encode = [\"Region\", \"Major\", \"Employment\"]   \n",
    "\n",
    "# Create empty lists to hold the StringIndexers and Encoders\n",
    "string_indexers = []\n",
    "encoders = []\n",
    "\n",
    "# Loop through each column and create a StringIndexer and an Encoder for it\n",
    "for col in columns_to_encode:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vector\")\n",
    "    string_indexers.append(indexer)\n",
    "    encoders.append(encoder)\n",
    "\n",
    "# Create a pipeline with StringIndexers and Encoders\n",
    "pipeline = Pipeline(stages=string_indexers + encoders)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = pipeline.fit(oversampled_df)\n",
    "encoded_df = model.transform(oversampled_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "encoded_df.show(n=2,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " Year                                          | 2019                  \n",
      " Hobbyist                                      | Yes                   \n",
      " ConvertedComp                                 | 95179.0               \n",
      " Employment                                    | Employed full-time    \n",
      " YearsCodePro                                  | 4                     \n",
      " Data scientist or machine learning specialist | 0                     \n",
      " Database administrator                        | 1                     \n",
      " Data or business analyst                      | 0                     \n",
      " Engineer, data                                | 0                     \n",
      " Region                                        | Oceania               \n",
      " Target                                        | 1                     \n",
      " EducationLevel                                | Undergraduate         \n",
      " Major                                         | STEM                  \n",
      " OrganizationSize                              | 11 - 50               \n",
      " encoded_EducationLevel                        | 2.0                   \n",
      " encoded_OrganizationSize                      | 0.0                   \n",
      " encoded_Hobbyist                              | 0.0                   \n",
      " encoded_Major                                 | 0.0                   \n",
      " encoded_Employment                            | 0.0                   \n",
      " encoded_Region                                | 3.0                   \n",
      " Region_index                                  | 3.0                   \n",
      " Major_index                                   | 0.0                   \n",
      " Employment_index                              | 0.0                   \n",
      " Region_vector                                 | (4,[3],[1.0])         \n",
      " Major_vector                                  | (4,[0],[1.0])         \n",
      " Employment_vector                             | (2,[0],[1.0])         \n",
      " ConvertedComp_vector                          | [95179.0]             \n",
      " YearsCodePro_vector                           | [4.0]                 \n",
      " encoded_EducationLevel_vector                 | [2.0]                 \n",
      " encoded_OrganizationSize_vector               | [0.0]                 \n",
      " ConvertedComp_scaled                          | [0.31900923396624825] \n",
      " YearsCodePro_scaled                           | [0.10344827586206896] \n",
      " encoded_EducationLevel_scaled                 | [0.2857142857142857]  \n",
      " encoded_OrganizationSize_scaled               | [0.0]                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List of columns to scale\n",
    "columns_to_scale = [\"ConvertedComp\", \"YearsCodePro\", \"encoded_EducationLevel\", \"encoded_OrganizationSize\"]\n",
    "\n",
    "assemblers = []\n",
    "scalers = []\n",
    "\n",
    "for col in columns_to_scale:\n",
    "    # Assemble the features into a vector\n",
    "    assembler = VectorAssembler(inputCols=[col], outputCol=col + \"_vector\")\n",
    "    scaler = MinMaxScaler(inputCol=col + \"_vector\", outputCol=col + \"_scaled\")\n",
    "    assemblers.append(assembler)\n",
    "    scalers.append(scaler)\n",
    "\n",
    "# Create pipeline\n",
    "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = scaling_pipeline.fit(encoded_df)\n",
    "scaled_df = model.transform(encoded_df)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "scaled_df.show(n=1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------------------------------\n",
      " Data scientist or machine learning specialist | 0                     \n",
      " Database administrator                        | 1                     \n",
      " Data or business analyst                      | 0                     \n",
      " Engineer, data                                | 0                     \n",
      " Target                                        | 1                     \n",
      " encoded_Hobbyist                              | 0.0                   \n",
      " Region_vector                                 | (4,[3],[1.0])         \n",
      " Major_vector                                  | (4,[0],[1.0])         \n",
      " Employment_vector                             | (2,[0],[1.0])         \n",
      " ConvertedComp_scaled                          | [0.31900923396624825] \n",
      " YearsCodePro_scaled                           | [0.10344827586206896] \n",
      " encoded_EducationLevel_scaled                 | [0.2857142857142857]  \n",
      " encoded_OrganizationSize_scaled               | [0.0]                 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"Year\", \"Hobbyist\", \"ConvertedComp\", \"Employment\", \"YearsCodePro\", \"Region\", \"EducationLevel\", \n",
    "                   \"Major\", \"OrganizationSize\", \"encoded_EducationLevel\", \"encoded_EducationLevel_vector\", \"encoded_Region\",\n",
    "                   \"encoded_OrganizationSize_vector\", \"encoded_OrganizationSize\", \"Region_index\",\"ConvertedComp_vector\", \n",
    "                   \"YearsCodePro_vector\", \"Employment_index\", \"Major_index\", \"encoded_Employment\", \"encoded_Major\"]\n",
    "\n",
    "spark_df = scaled_df.drop(*columns_to_drop)\n",
    "spark_df.show(n=1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------------------------------------------------------------------------------------------------\n",
      " features | (19,[1,8,9,13,15,16,17],[1.0,1.0,1.0,1.0,0.31900923396624825,0.10344827586206896,0.2857142857142857])                     \n",
      " Target   | 1                                                                                                                         \n",
      "-RECORD 1-----------------------------------------------------------------------------------------------------------------------------\n",
      " features | (19,[0,1,2,7,9,13,15,16,17,18],[1.0,1.0,1.0,1.0,1.0,1.0,0.044550954399959784,0.3103448275862069,0.14285714285714285,0.8]) \n",
      " Target   | 0                                                                                                                         \n",
      "-RECORD 2-----------------------------------------------------------------------------------------------------------------------------\n",
      " features | (19,[1,2,3,6,9,13,15,16,17],[1.0,1.0,1.0,1.0,1.0,1.0,0.30165071810427174,0.24137931034482757,0.14285714285714285])        \n",
      " Target   | 1                                                                                                                         \n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a VectorAssembler to be passed to our models\n",
    "feature_cols = [c for c in spark_df.columns if c != 'Target']\n",
    "assembler = VectorAssembler(inputCols=feature_cols , outputCol=\"features\")\n",
    "\n",
    "# Use transform to assemble the columns\n",
    "assembled_df = assembler.transform(spark_df)  \n",
    "\n",
    "selected_columns = [\"features\", \"Target\"]\n",
    "assembled_df = assembled_df.select(selected_columns)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "assembled_df.show(n=3, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-------------------------------------------------------------------------------------------------------------------------------\n",
      " features | (19,[0,1,2,3,4,5,9,13,15,16,17],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.2880126024366946,0.4482758620689655,0.2857142857142857]) \n",
      " Target   | 1                                                                                                                           \n",
      "only showing top 1 row\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 8072|\n",
      "|     0| 8056|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 1992|\n",
      "|     0| 1943|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 2451|\n",
      "|     0| 2501|\n",
      "+------+-----+\n",
      "\n",
      "16128\n",
      "3935\n",
      "4952\n"
     ]
    }
   ],
   "source": [
    "# holdout validation\n",
    "(train, test) = assembled_df.randomSplit([0.8, 0.2], seed = 5)\n",
    "(train_df, val_df) = train.randomSplit([0.8, 0.2], seed = 5)\n",
    "\n",
    "# Show a sample of the training set\n",
    "train.show(n=1, vertical=True, truncate=False)\n",
    "\n",
    "# Show the total number of samples for each class label per set\n",
    "train_df.groupBy('Target').count().show()\n",
    "val_df.groupBy('Target').count().show()\n",
    "test.groupBy('Target').count().show()\n",
    "\n",
    "# Show total number of samples per set\n",
    "print(train_df.count())\n",
    "print(val_df.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.1 Random Forest Using All Categorical Features:\n",
    "\n",
    "We will use the same hyperparameters values that we employed when we trained Random Forest using only three categorical features in the last trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "start4 = time.time()\n",
    "\n",
    "rf_tuning4 = pd.DataFrame(columns=['numTrees', 'maxDepth', 'maxBins', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 1):\n",
    "\n",
    "    # Randomly select values for hyperparameters from a range\n",
    "    trees4 = random.randrange(5, 61, 5)\n",
    "    depth4 = random.randrange(2, 21, 2)\n",
    "    bins4 = random.randrange(10, 61, 5)\n",
    "\n",
    "    # Create a RandomForestClassifier instance\n",
    "    randomForestClassifier4 = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features\", numTrees=trees4,\n",
    "                                                     maxDepth=depth4, maxBins=bins4)\n",
    "\n",
    "    randomForestModel4 = randomForestClassifier4.fit(train_df)\n",
    "\n",
    "    # // run model with test data set to get predictions\n",
    "    predictions4 = randomForestModel4.transform(val_df)\n",
    "    predictions4.select(\"Target\", \"prediction\").show()\n",
    "    \n",
    "end4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) for random forest classifier tuning: 1.0066583116849264\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) for random forest classifier tuning:', (end4-start4)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.6977848255582049\n",
      "Validation Error = 0.30221517444179513\n"
     ]
    }
   ],
   "source": [
    "evaluator4 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "accuracy4 = evaluator4.evaluate(predictions4)\n",
    "print(\"Accuracy = %s\" % (accuracy4))\n",
    "print(\"Validation Error = %s\" % (1.0 - accuracy4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.2 Naive Bayes Using All Categorical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_start4 = time.time()\n",
    "\n",
    "# Create a NaiveBayes classifier\n",
    "nb4 = NaiveBayes(labelCol=\"Target\", featuresCol=\"features\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "nb_model4 = nb4.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "nb_predictions4 = nb_model4.transform(val_df)\n",
    "nb_predictions4.select(\"Target\", \"prediction\").show()\n",
    "\n",
    "nb_end4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) to fit Naive Bayes model and make predictions: 0.0601337989171346\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) to fit Naive Bayes model and make predictions:', (nb_end4-nb_start4)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5395214456411287\n",
      "Validation Error = 0.4604785543588713\n"
     ]
    }
   ],
   "source": [
    "nb_evaluator4 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "nb_accuracy4 = nb_evaluator4.evaluate(nb_predictions4)\n",
    "print(\"Accuracy = %s\" % (nb_accuracy4))\n",
    "print(\"Validation Error = %s\" % (1.0 - nb_accuracy4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5.3 Logistic Regression Using All Categorical Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-2ad2cc2a5364>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning4 = lr_tuning4.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-2ad2cc2a5364>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning4 = lr_tuning4.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-2ad2cc2a5364>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning4 = lr_tuning4.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-2ad2cc2a5364>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning4 = lr_tuning4.append({'regParam': reg, 'iterations': iters,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "|     0|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       0.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-134-2ad2cc2a5364>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  lr_tuning4 = lr_tuning4.append({'regParam': reg, 'iterations': iters,\n"
     ]
    }
   ],
   "source": [
    "lr_start4 = time.time()\n",
    "\n",
    "lr_tuning4 = pd.DataFrame(columns=['regParam', 'iterations', 'TP', 'TN', 'FP', 'FN'])\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    # Create a LogisticRegression classifier\n",
    "    reg = random.uniform(0, 0.05)\n",
    "    iters = random.randrange(10, 101, 5)\n",
    "\n",
    "    # Create a LogisticRegression instance with the selected hyperparameters\n",
    "    lr4 = LogisticRegression(labelCol=\"Target\", featuresCol=\"features\", regParam=reg, maxIter=iters, elasticNetParam=1.0,\n",
    "                            family=\"auto\")\n",
    "    \n",
    "    # Fit the model to the training data\n",
    "    lr_model4 = lr4.fit(train_df)\n",
    "    # Make predictions on the validation data\n",
    "    lr_predictions4 = lr_model4.transform(val_df)\n",
    "    lr_predictions4.select(\"Target\", \"prediction\").show()\n",
    "    \n",
    "lr_end4 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in minutes) to fit Logistic Regression model and make predictions: 0.9122393290201823\n"
     ]
    }
   ],
   "source": [
    "print('Time taken (in minutes) to fit Logistic Regression model and make predictions:', (lr_end4-lr_start4)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.5513726584504396\n",
      "Validation Error = 0.4486273415495604\n"
     ]
    }
   ],
   "source": [
    "# Define the MulticlassClassificationEvaluator\n",
    "lr_evaluator4 = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "\n",
    "# Calculate the accuracy\n",
    "lr_accuracy4 = lr_evaluator4.evaluate(lr_predictions4)\n",
    "\n",
    "print(\"Accuracy = %s\" % (lr_accuracy4))\n",
    "print(\"Validation Error = %s\" % (1.0 - lr_accuracy4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice from the above section, after we trained our three models using all our categorical features, that their performance has been decreased in terms of accuracy and time efficiency. The accuracy for all the models has decreased and the time taken to complete the training process increased as well. Although the performance has slightly worsened, it's important to note that the future surveys spanning from 2021 and beyond, are likely to have more variables leading to a remarkable risk if we didn't employ feature selection. \n",
    "\n",
    "\n",
    "Therefore, as it's always recommended to opt for simpler models rather than complex ones to guarantee optimal performance and to increase the interpretability of the models, we will proceed in the next section using only the highly significant categorical variables resulting from the chi-square test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features we are going to proceed with are: \n",
      "-RECORD 0--------------------------------------------------------------\n",
      " Data scientist or machine learning specialist | 0                     \n",
      " Database administrator                        | 1                     \n",
      " Data or business analyst                      | 0                     \n",
      " Engineer, data                                | 0                     \n",
      " Target                                        | 1                     \n",
      " Region_vector                                 | (4,[3],[1.0])         \n",
      " ConvertedComp_scaled                          | [0.31900923396624825] \n",
      " YearsCodePro_scaled                           | [0.10344827586206896] \n",
      " encoded_EducationLevel_scaled                 | [0.2857142857142857]  \n",
      " encoded_OrganizationSize_scaled               | [0.0]                 \n",
      "only showing top 1 row\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 7988|\n",
      "|     0| 8140|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 2006|\n",
      "|     0| 1929|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|Target|count|\n",
      "+------+-----+\n",
      "|     1| 2521|\n",
      "|     0| 2431|\n",
      "+------+-----+\n",
      "\n",
      "16128\n",
      "3935\n",
      "4952\n"
     ]
    }
   ],
   "source": [
    "# Define your categorical features\n",
    "categorical_features = ['OrganizationSize','Region', 'Hobbyist', 'EducationLevel', 'Employment', 'Major']\n",
    "\n",
    "# Initialize a StringIndexer\n",
    "stringIndexer = StringIndexer(inputCols=categorical_features, outputCols=[\"encoded_\" + col for col in categorical_features])\n",
    "\n",
    "# Fit the StringIndexer\n",
    "model = stringIndexer.fit(dataframe_cleaned)\n",
    "\n",
    "# Transform the data\n",
    "dataframe_with_encoded_features = model.transform(dataframe_cleaned)\n",
    "\n",
    "# Create a VectorAssembler to assemble the features into a DenseVector\n",
    "encoded_features = [\"encoded_OrganizationSize\", \"encoded_Region\", \"encoded_Hobbyist\", \"encoded_EducationLevel\", \"encoded_Employment\", \"encoded_Major\"]\n",
    "\n",
    "# the result is in \"features\" columns\n",
    "assembler = VectorAssembler(inputCols=encoded_features, outputCol=\"features\")\n",
    "\n",
    "selector = ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=\"Target\", fpr= 0.05)\n",
    "\n",
    "# Create a pipeline consists of assembler followed by the ChiSqSelector\n",
    "pipeline = Pipeline(stages=[assembler, selector])\n",
    "\n",
    "# Fit the pipeline\n",
    "model = pipeline.fit(dataframe_with_encoded_features)\n",
    "\n",
    "# Transform the data\n",
    "result = model.transform(dataframe_with_encoded_features)\n",
    "\n",
    "# The selectedFeatures column now contains the most important features\n",
    "selected_features = result.select(\"selectedFeatures\")\n",
    "\n",
    "# Define a dictionary to map selected feature indices to variable names\n",
    "feature_mapping = {\n",
    "    0: \"encoded_OrganizationSize\",\n",
    "    1: \"encoded_Region\",\n",
    "    2: \"encoded_Hobbyist\",\n",
    "    3: \"encoded_EducationLevel\",\n",
    "    4: \"encoded_Employment\",\n",
    "    5: \"encoded_Major\"\n",
    "}\n",
    "\n",
    "# Extract the selected feature indices from the DataFrame\n",
    "selected_feature_indices = result.select(\"selectedFeatures\").collect()[0][0].toArray()\n",
    "\n",
    "# Sort the indices based on their values (Chi-Square statistics)\n",
    "sorted_indices = sorted(range(len(selected_feature_indices)), key=lambda i: selected_feature_indices[i], reverse=True)\n",
    "\n",
    "# Select the top 3 feature indices\n",
    "top_3_indices = sorted_indices[:3]\n",
    "\n",
    "# Map selected feature indices to variable names\n",
    "selected_features = [feature_mapping[i] for i in top_3_indices]\n",
    "\n",
    "\n",
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Hobbyist)\n",
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Employment)\n",
    "dataframe_with_encoded_features = dataframe_with_encoded_features.drop(dataframe_with_encoded_features.encoded_Major)\n",
    "\n",
    "counts = dataframe_with_encoded_features.groupBy('Target').count()\n",
    "\n",
    "\n",
    "# Get the count for the 0 and 1 target labels\n",
    "count_0 = counts.filter(counts['Target'] == 0).select('count').collect()[0][0]\n",
    "count_1 = counts.filter(counts['Target'] == 1).select('count').collect()[0][0]\n",
    "\n",
    "# Calculate the oversampling fraction for the 0 target label\n",
    "oversampling_fraction_0 = (count_1/2.12) / count_0\n",
    "\n",
    "# Create a DataFrame with additional rows for the 0 target label\n",
    "oversampled_df = dataframe_with_encoded_features.unionAll(\n",
    "    dataframe_with_encoded_features.filter(dataframe_with_encoded_features['Target'] == 0)\n",
    "    .sample(True, oversampling_fraction_0 , seed=200)\n",
    "    .withColumn('Target', lit(0))\n",
    ")\n",
    "\n",
    "# Define the list of columns to be one-hot encoded\n",
    "columns_to_encode = [\"Region\"]   \n",
    "\n",
    "# Create empty lists to hold the StringIndexers and Encoders\n",
    "string_indexers = []\n",
    "encoders = []\n",
    "\n",
    "# Loop through each column and create a StringIndexer and an Encoder for it\n",
    "for col in columns_to_encode:\n",
    "    indexer = StringIndexer(inputCol=col, outputCol=col + \"_index\")\n",
    "    encoder = OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vector\")\n",
    "    string_indexers.append(indexer)\n",
    "    encoders.append(encoder)\n",
    "\n",
    "# Create a pipeline with StringIndexers and Encoders\n",
    "pipeline = Pipeline(stages=string_indexers + encoders)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = pipeline.fit(oversampled_df)\n",
    "encoded_df = model.transform(oversampled_df)\n",
    "\n",
    "\n",
    "# List of columns to scale\n",
    "columns_to_scale = [\"ConvertedComp\", \"YearsCodePro\", \"encoded_EducationLevel\", \"encoded_OrganizationSize\"]\n",
    "\n",
    "assemblers = []\n",
    "scalers = []\n",
    "\n",
    "for col in columns_to_scale:\n",
    "    # Assemble the features into a vector\n",
    "    assembler = VectorAssembler(inputCols=[col], outputCol=col + \"_vector\")\n",
    "    scaler = MinMaxScaler(inputCol=col + \"_vector\", outputCol=col + \"_scaled\")\n",
    "    assemblers.append(assembler)\n",
    "    scalers.append(scaler)\n",
    "\n",
    "# Create pipeline\n",
    "scaling_pipeline = Pipeline(stages=assemblers + scalers)\n",
    "\n",
    "# Fit and transform the data using the pipeline\n",
    "model = scaling_pipeline.fit(encoded_df)\n",
    "scaled_df = model.transform(encoded_df)\n",
    "\n",
    "columns_to_drop = [\"Year\", \"Hobbyist\", \"ConvertedComp\", \"Employment\", \"YearsCodePro\", \"Region\", \"EducationLevel\", \n",
    "                   \"Major\", \"OrganizationSize\", \"encoded_EducationLevel\", \"encoded_EducationLevel_vector\", \"encoded_Region\",\n",
    "                   \"encoded_OrganizationSize_vector\", \"encoded_OrganizationSize\", \"Region_index\",\"ConvertedComp_vector\", \n",
    "                   \"YearsCodePro_vector\"]\n",
    "\n",
    "spark_df = scaled_df.drop(*columns_to_drop)\n",
    "print(\"The features we are going to proceed with are: \")\n",
    "spark_df.show(n=1, vertical=True, truncate=False)\n",
    "\n",
    "# Create a VectorAssembler to be passed to our models\n",
    "feature_cols = [c for c in spark_df.columns if c != 'Target']\n",
    "assembler = VectorAssembler(inputCols=feature_cols , outputCol=\"features\")\n",
    "\n",
    "# Use transform to assemble the columns\n",
    "assembled_df = assembler.transform(spark_df)  \n",
    "\n",
    "selected_columns = [\"features\", \"Target\"]\n",
    "assembled_df = assembled_df.select(selected_columns)\n",
    "\n",
    "\n",
    "# holdout validation\n",
    "(train, test) = assembled_df.randomSplit([0.8, 0.2], seed = 5)\n",
    "(train_df, val_df) = train.randomSplit([0.8, 0.2], seed = 5)\n",
    "\n",
    "\n",
    "# Show the total number of samples for each class label per set\n",
    "train_df.groupBy('Target').count().show()\n",
    "val_df.groupBy('Target').count().show()\n",
    "test.groupBy('Target').count().show()\n",
    "\n",
    "# Show total number of samples per set\n",
    "print(train_df.count())\n",
    "print(val_df.count())\n",
    "print(test.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.9 Predictions on Test Set Using Random Forest\n",
    "\n",
    "The random forest model is the best-performing model as it achieved the highest accuracy, which is 69%, on our balanced dataset during the training and validation phases with trees of 61, a max depth of 20, and a max bin of 60 using the resulting categorical features from chi-square test. Therefore, we will train this model on our fully balanced training dataset and then compute the accuracy using the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Target|prediction|\n",
      "+------+----------+\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       0.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     0|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       1.0|\n",
      "|     1|       0.0|\n",
      "|     1|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Accuracy = 0.7211649005537517\n",
      "Test Error = 0.2788350994462483\n"
     ]
    }
   ],
   "source": [
    "randomForestClassifier_test = RandomForestClassifier(labelCol=\"Target\", featuresCol=\"features\",\n",
    "                                                     numTrees=60, maxDepth=20, maxBins=60)\n",
    "\n",
    "randomForestModel_test = randomForestClassifier_test.fit(train)\n",
    "\n",
    "predictions_test = randomForestModel_test.transform(test)\n",
    "    \n",
    "predictions_test.select(\"Target\", \"prediction\").show()\n",
    "\n",
    "prediction_targets_test = pd.DataFrame({'targets': predictions_test.select(\"Target\").collect(), 'predictions': predictions_test.select(\"prediction\").collect()})\n",
    "\n",
    "evaluator_test = MulticlassClassificationEvaluator(labelCol=\"Target\", predictionCol=\"prediction\")\n",
    "\n",
    "accuracy_test = evaluator_test.evaluate(predictions_test)\n",
    "\n",
    "print(\"Accuracy = %s\" % (accuracy_test))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Summary and Conclusion\n",
    "\n",
    "#### 3.1 Summary:\n",
    "\n",
    "All in all, we have processed the results from the \"Stack Overflow Annual Developers Survey\" from 2017 to 2020 to predict whether the employee is satisfied with his job. We started by exploring the dataset in the EDA section by getting familiar with our numerical and categorical variables, followed by the hypothesis testing section. The target of the hypothesis testing is to identify which categorical factors affect employee satisfaction, and it was illustrated that the *Region*, *Education Level*, and *Organization Size* contribute to the prediction of employee satisfaction. Furthermore, we pre-processed our dataset by balancing it using an oversampling technique due to having an imbalanced target variable, applying one-hot encoding to the categorical features, and then scaling the numerical features. The last step in the pre-processing phase was dropping the irrelevant features from our analysis and creating a vector assembler to be passed to our models. \n",
    "\n",
    "At this point, we are ready to build our predictive models, but first, we folded the dataset into three folds: training, validation, and testing sets. Next, we started running our random forest model, followed by naive bayes and logistic regression models using the training and validation sets. However, the results were not satisfying, so we considered all the categorical variables instead of the only three resulting from the chi-square test to compare the models' performance in terms of accuracy and time efficiency. Nevertheless, the performance degradation was relatively modest. Moreover, the models' complexity has increased, affecting their performance and interpretability. Thus, we continued our analysis but only used the resulting features from the chi-square test. \n",
    "\n",
    "#### 3.2 Goals:\n",
    "\n",
    "Our target is to build a suitable Spark ML model to predict whether the employee is satisfied or not with his job by making the best use of the provided capabilities: HDFS and Apache Spark. \n",
    "\n",
    "#### 3.3 Challenges And Results:\n",
    "\n",
    "The process of identifying which model to proceed with was not easy, it can be considered one of the challenges that we have encountered during our big data analysis journey. We can see that when using the accuracy metric on the Naive Bayes algorithm, considering the three categorical features resulting from the chi-square test yields a result of 55%, while using all the categorical features yields 53%. We believe that it is the worst-performing model due to its naive assumptions. Regarding the Logistic Regression model, it yielded a very low accuracy, which is 56% using the categorical features resulting from the chi-square test and 55% using all the categorical features. \n",
    "\n",
    "We also see that the Random Forest yielded the highest accuracy, with a result of 69% using both datasets, resulting from the chi-square test and the whole categorical features. Interestingly, both datasets performed equally well in terms of accuracy with the random forest, however, the time taken to train our model increased when we considered all our categorical features, and this is due to the increased model's complexity. This means that the algorithm was able to classify regardless of the extra features added. This is what makes the random forest classifier the best-suited algorithm since it performs with high accuracy, avoids bias and overfitting, and can handle large amounts of features and data so it will predict/classify the output accurately. Lastly, we applied random forest to our test set, and it achieved an accuracy of 72%, which is high compared to the previously mentioned models. We expected to get a higher value, but this could be due to our second challenge, hyperparameter tuning.\n",
    "\n",
    "\n",
    "It's worth mentioning that the performance of our models could be better, however, this could be due to our hyperparameter tuning process needing to be more robust, such as the grid search method. Unfortunately, this was limited by how many random searches we could perform. The number of random searches was limited, specifically while running the random forest algorithm, as an error occurred when we increased the number of trees and the maximum tree depth during a large number of random searches, leading to a \"Java heap space\" error, indicating an out-of-memory issue that required session restarts.\n",
    "\n",
    "During the first trial to tune our random forest model, using a tree number of 200 and a maximum depth of 20, the time consumed was approximately 8 minutes, which is relatively high compared to the last attempt. In the last attempt, while tuning the random forest algorithm, the time consumed was 0.70 minutes with a tree number of 60 and a maximum depth of 20. Moving to the time consumed by the Logistic Regression model, while applying hyperparameter tuning, it was 0.72 minutes, which is relatively close to the time taken during the last attempt of the random forest model but with a lower accuracy result.\n",
    "\n",
    "The mentioned parameters of the random forest model play a huge role in incrementing the training and testing time of the model, so for example, the more trees the slower our learner is, which will increase the accuracy. The same applies to the maximum depth, if the number of levels increases per tree, this will increase the training time for each tree, hence the whole forest.\n",
    "\n",
    "#### 3.4 Conclusion:\n",
    "\n",
    "To conclude, our project is transferable to different domain-specific areas other than merely predicting employee satisfaction in the field of data analytics, as it could be utilized in a wide array of different aspects of different fields. Our model could be replicated using different programming languages, such as Scala which could be a task for the future. Also, more complex algorithms, such as deep learning techniques, could capture the high dimensionality of the data and provide better metrics. The downside of these algorithms is their black box techniques, which could be difficult to grasp and explain to the business side. Also, deep learning requires high computational power, making it an optimal choice to take advantage of the available capabilities within HDFS and Apache Spark. \n",
    "\n",
    "\n",
    "Thus, we propose for future reports and analyses to try to incorporate other hyperparameter tuning techniques and maybe explore which ensemble methods such as Xg-Boost and AdaBoost may perform better than random forests for such datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. References:\n",
    "\n",
    "\n",
    "1. Allibhai, E. (2018). Holdout vs. cross-validation in machine learning. [online] Medium. URL: https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f.\n",
    "\n",
    "\n",
    "2. Avinash Navlani (2018). Random Forests Classifiers in Python. [online] DataCamp Community. URL: https://www.datacamp.com/community/tutorials/random-forests-classifier-python.\n",
    "\n",
    "\n",
    "3. Biswal, A. (2023). What is a Chi-Square Test? Formula, Examples & Uses | Simplilearn. [online] Simplilearn.com. URL: https://www.simplilearn.com/tutorials/statistics-tutorial/chi-square-test.\n",
    "\n",
    "\n",
    "4. Brownlee, J. (2019). How to Choose a Feature Selection Method For Machine Learning. [online] Machine Learning Mastery. URL: https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/.\n",
    "\n",
    "\n",
    "5. Brownlee, J. (2019). How to Perform Feature Selection with Categorical Data. [online] MachineLearningMastery.com. URL: https://machinelearningmastery.com/feature-selection-with-categorical-data/.\n",
    "\n",
    "\n",
    "6. Chauhan, N., 2022. Na√Øve Bayes Algorithm: Everything You Need to Know - KDnuggets. [Blog] KDnuggets. URL: <https://www.kdnuggets.com/2020/06/naive-bayes-algorithm-everything.html#:~:text=The%20Zero%2DFrequency%20Problem,all%20the%20probabilities%20are%20multiplied.>\n",
    "\n",
    "\n",
    "7. Eijaz Allibhai (2018). Holdout vs. Cross-validation in Machine Learning. [online] Medium. URL: https://medium.com/@eijaz/holdout-vs-cross-validation-in-machine-learning-7637112d3f8f.\n",
    "\n",
    "\n",
    "8. Goswami, D.S. (2020). Using the Chi-Squared test for feature selection with implementation. [online] Medium. URL: https://towardsdatascience.com/using-the-chi-squared-test-for-feature-selection-with-implementation-b15a4dad93f1.\n",
    "\n",
    "\n",
    "9. Irfan, S., 2021. Hyperparameter Tuning in Random Forest. [online] Medium. URL: <https://medium.com/geekculture/random-forest-algorithm-has-proven-to-be-one-of-the-most-sought-after-algorithm-in-the-field-of-295da606bf9>\n",
    "\n",
    "\n",
    "10. Koehrsen, W., 2018. Hyperparameter Tuning the Random Forest in Python. [online] Medium. URL: <https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74>\n",
    "\n",
    "\n",
    "11. Lindgren, I. (2019). Transformations, Scaling and Normalization. [online] Medium. URL: https://medium.com/@isalindgren313/transformations-scaling-and-normalization-420b2be12300.\n",
    "\n",
    "\n",
    "12. Logunova, I., 2022. Guide to Random Forest Classification and Regression Algorithms. [online] Serokell Software Development Company. URL: <https://serokell.io/blog/random-forest-classification>\n",
    "\n",
    "\n",
    "13. MALATO, G., 2021. Feature selection with Random Forest | Your Data Teacher. [online] Your Data Teacher. URL: <https://www.yourdatateacher.com/2021/10/11/feature-selection-with-random-forest/>\n",
    "\n",
    "\n",
    "14. Mazumder, S., 2021. What is Imbalanced Data | Techniques to Handle Imbalanced Data. [online] Analytics Vidhya. URL: <https://www.analyticsvidhya.com/blog/2021/06/5-techniques-to-handle-imbalanced-data-for-a-classification-problem/>\n",
    "\n",
    "\n",
    "15. Navlani, A., 2018. Naive Bayes Classification Tutorial using Scikit-learn. [Blog] datacamp, URL: <https://www.datacamp.com/tutorial/naive-bayes-scikit-learn>\n",
    "\n",
    "\n",
    "16. Priyanjalee, M. (2021). A Guide to exploit Random Forest Classifier in PySpark. [online] Medium. URL: https://towardsdatascience.com/a-guide-to-exploit-random-forest-classifier-in-pyspark-46d6999cb5db.\n",
    "\n",
    "\n",
    "17. R, S., 2021. Random Forest | Introduction to Random Forest Algorithm. [online] Analytics Vidhya. URL: <https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/>\n",
    "\n",
    "\n",
    "18. Shrestha, S. (2021). Preparing Data for Apache Spark ML Model. [online] Medium. URL: https://medium.com/@statistics.sudip/preparing-data-for-apache-spark-ml-model-4fedcc31a0f4.\n",
    "\n",
    "\n",
    "19. spark.apache.org. (n.d.). Extracting, transforming and selecting features - Spark 2.2.0 Documentation. [online] URL: https://spark.apache.org/docs/2.2.0/ml-features.html#chisqselector.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
